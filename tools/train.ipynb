{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共有 8 个CUDA设备\n",
      "设备 0:\n",
      "  名称: NVIDIA GeForce RTX 3090\n",
      "  计算能力: 8.6\n",
      "  总内存: 23.69 GB\n",
      "设备 1:\n",
      "  名称: NVIDIA GeForce RTX 3090\n",
      "  计算能力: 8.6\n",
      "  总内存: 23.69 GB\n",
      "设备 2:\n",
      "  名称: NVIDIA GeForce RTX 3090\n",
      "  计算能力: 8.6\n",
      "  总内存: 23.69 GB\n",
      "设备 3:\n",
      "  名称: NVIDIA GeForce RTX 3090\n",
      "  计算能力: 8.6\n",
      "  总内存: 23.69 GB\n",
      "设备 4:\n",
      "  名称: NVIDIA GeForce RTX 3090\n",
      "  计算能力: 8.6\n",
      "  总内存: 23.69 GB\n",
      "设备 5:\n",
      "  名称: NVIDIA GeForce RTX 3090\n",
      "  计算能力: 8.6\n",
      "  总内存: 23.69 GB\n",
      "设备 6:\n",
      "  名称: NVIDIA GeForce RTX 3090\n",
      "  计算能力: 8.6\n",
      "  总内存: 23.69 GB\n",
      "设备 7:\n",
      "  名称: NVIDIA GeForce RTX 3090\n",
      "  计算能力: 8.6\n",
      "  总内存: 23.69 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 检查是否有可用的CUDA设备\n",
    "if torch.cuda.is_available():\n",
    "    # 获取设备数量\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    print(f\"共有 {num_devices} 个CUDA设备\")\n",
    "\n",
    "    # 遍历每个设备并打印其信息\n",
    "    for i in range(num_devices):\n",
    "        device_name = torch.cuda.get_device_name(i)\n",
    "        device_capability = torch.cuda.get_device_capability(i)\n",
    "        device_memory = torch.cuda.get_device_properties(i).total_memory / (1024 ** 3)  # 以GB为单位\n",
    "        print(f\"设备 {i}:\")\n",
    "        print(f\"  名称: {device_name}\")\n",
    "        print(f\"  计算能力: {device_capability[0]}.{device_capability[1]}\")\n",
    "        print(f\"  总内存: {device_memory:.2f} GB\")\n",
    "else:\n",
    "    print(\"没有可用的CUDA设备\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/comp_robot/yangyuqin/Anaconda3/envs/deepl2/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from net import MnistNet\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from dataloader import MNISTDataloader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "epochs = 20\n",
    "device_torch = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = MnistNet()\n",
    "net = net.to(device_torch)\n",
    "optimizer = optim.Adam(net.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练函数\n",
    "def train(model, device, train_loader, optimizer):\n",
    "    # model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)  # 将数据和标签转移到GPU/CPU\n",
    "        # print(\"data: \", data.shape)\n",
    "        # print(\"target: \", target.shape)\n",
    "        optimizer.zero_grad()  # 清空上一步的梯度\n",
    "        output = model.forward(data)  # 前向传播\n",
    "        # print(\"output: \", output.shape)\n",
    "        loss = F.nll_loss(output, target)  # 计算损失\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step()  # 更新参数\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Batch: {batch_idx}, Loss: {loss.item()}')\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model.forward(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.max(1, keepdim=True)[1] # 找到概率最大的下标\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    " \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    acc = correct / len(test_loader.dataset) * 100.\n",
    "    print('\\n验证集: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct,\n",
    "        len(test_loader.dataset), acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2745188/1020990586.py:18: FutureWarning: The input object of type 'PngImageFile' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'PngImageFile', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  train_images = np.array(dataset['train']['image'])\n",
      "/tmp/ipykernel_2745188/1020990586.py:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_images = np.array(dataset['train']['image'])\n",
      "/tmp/ipykernel_2745188/1020990586.py:21: FutureWarning: The input object of type 'PngImageFile' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'PngImageFile', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  test_images = np.array(dataset['test']['image'])\n",
      "/tmp/ipykernel_2745188/1020990586.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  test_images = np.array(dataset['test']['image'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataLoader: 938\n",
      "Test DataLoader: 157\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This file is used to download andpreprocess the dataset for training and testing.\n",
    "\n",
    "# 数据集路径\n",
    "dataset_root_path = \"/comp_robot/yangyuqin/workplace/startup/dataset/MNIST/raw\"\n",
    "dataset_train_path = os.path.join(dataset_root_path, 'ylecun/mnist/mnist/train-00000-of-00001.parquet')\n",
    "dataset_test_path = os.path.join(dataset_root_path, 'ylecun/mnist/mnist/test-00000-of-00001.parquet')\n",
    "\n",
    "# 图像预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "dataset = load_dataset('parquet', data_files={'train': dataset_train_path, 'test': dataset_test_path})\n",
    "\n",
    "# 提取数据\n",
    "train_images = np.array(dataset['train']['image'])\n",
    "train_labels = np.array(dataset['train']['label'])\n",
    "\n",
    "test_images = np.array(dataset['test']['image'])\n",
    "test_labels = np.array(dataset['test']['label'])\n",
    "\n",
    "# 应用图像预处理\n",
    "train_images = np.stack([transform(image) for image in train_images])\n",
    "test_images = np.stack([transform(image) for image in test_images])\n",
    "\n",
    "# 转换为PyTorch张量\n",
    "train_images_tensor = torch.tensor(train_images, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "test_images_tensor = torch.tensor(test_images, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "train_dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
    "test_dataset = TensorDataset(test_images_tensor, test_labels_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 打印一些信息来验证\n",
    "print(\"Train DataLoader:\", len(train_loader))\n",
    "print(\"Test DataLoader:\", len(test_loader))\n",
    "\n",
    "\n",
    "# 定义数据加载器\n",
    "# train_loader, test_loader = MNISTDataloader(dataset)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Batch: 0, Loss: 0.00018155212455894798\n",
      "Batch: 4, Loss: 0.0014149120543152094\n",
      "Batch: 8, Loss: 0.0045260777696967125\n",
      "Batch: 12, Loss: 0.011960601434111595\n",
      "Batch: 16, Loss: 0.004531370475888252\n",
      "Batch: 20, Loss: 0.012264830060303211\n",
      "Batch: 24, Loss: 0.024474620819091797\n",
      "Batch: 28, Loss: 0.039758533239364624\n",
      "Batch: 32, Loss: 7.626802835147828e-05\n",
      "Batch: 36, Loss: 0.0010706980247050524\n",
      "Batch: 40, Loss: 0.0003390436468180269\n",
      "Batch: 44, Loss: 0.045172642916440964\n",
      "Batch: 48, Loss: 0.0013178333174437284\n",
      "Batch: 52, Loss: 0.01633531227707863\n",
      "Batch: 56, Loss: 0.023964939638972282\n",
      "Batch: 60, Loss: 0.002593002747744322\n",
      "Batch: 64, Loss: 0.0017826100811362267\n",
      "Batch: 68, Loss: 0.00036938406992703676\n",
      "Batch: 72, Loss: 0.0021321058738976717\n",
      "Batch: 76, Loss: 0.00036207467201165855\n",
      "Batch: 80, Loss: 0.0005656335270032287\n",
      "Batch: 84, Loss: 0.003970462828874588\n",
      "Batch: 88, Loss: 0.0001058786510839127\n",
      "Batch: 92, Loss: 0.0019468284444883466\n",
      "Batch: 96, Loss: 0.00023128108296077698\n",
      "Batch: 100, Loss: 0.003814917989075184\n",
      "Batch: 104, Loss: 0.0015446222387254238\n",
      "Batch: 108, Loss: 0.10028223693370819\n",
      "Batch: 112, Loss: 0.00969450268894434\n",
      "Batch: 116, Loss: 0.00020863751706201583\n",
      "Batch: 120, Loss: 0.036524027585983276\n",
      "Batch: 124, Loss: 0.0013540354557335377\n",
      "Batch: 128, Loss: 0.001347714918665588\n",
      "Batch: 132, Loss: 0.010680628940463066\n",
      "Batch: 136, Loss: 0.10688301175832748\n",
      "Batch: 140, Loss: 0.007418863475322723\n",
      "Batch: 144, Loss: 0.051478900015354156\n",
      "Batch: 148, Loss: 0.0010014136787503958\n",
      "Batch: 152, Loss: 0.05433562397956848\n",
      "Batch: 156, Loss: 0.0012631417484954\n",
      "Batch: 160, Loss: 0.001680109417065978\n",
      "Batch: 164, Loss: 0.003877036739140749\n",
      "Batch: 168, Loss: 0.0026075909845530987\n",
      "Batch: 172, Loss: 0.000800450099632144\n",
      "Batch: 176, Loss: 0.00022627445287071168\n",
      "Batch: 180, Loss: 0.0005018714582547545\n",
      "Batch: 184, Loss: 0.0011367659317329526\n",
      "Batch: 188, Loss: 0.0008498605457134545\n",
      "Batch: 192, Loss: 4.3668231228366494e-05\n",
      "Batch: 196, Loss: 0.14990796148777008\n",
      "Batch: 200, Loss: 4.4332195102469996e-05\n",
      "Batch: 204, Loss: 0.002525251591578126\n",
      "Batch: 208, Loss: 0.001094262464903295\n",
      "Batch: 212, Loss: 0.0020755333825945854\n",
      "Batch: 216, Loss: 5.50788281543646e-05\n",
      "Batch: 220, Loss: 0.0001648141333134845\n",
      "Batch: 224, Loss: 7.293730595847592e-05\n",
      "Batch: 228, Loss: 0.0003808956826105714\n",
      "Batch: 232, Loss: 6.3638377469033e-05\n",
      "Batch: 236, Loss: 0.0017532201018184423\n",
      "Batch: 240, Loss: 0.010858000256121159\n",
      "Batch: 244, Loss: 0.001045229728333652\n",
      "Batch: 248, Loss: 0.005854012910276651\n",
      "Batch: 252, Loss: 0.000783755851443857\n",
      "Batch: 256, Loss: 0.0010789488442242146\n",
      "Batch: 260, Loss: 0.0020939463283866644\n",
      "Batch: 264, Loss: 0.0034319343976676464\n",
      "Batch: 268, Loss: 0.0011276821605861187\n",
      "Batch: 272, Loss: 0.0015144299250096083\n",
      "Batch: 276, Loss: 0.009764519520103931\n",
      "Batch: 280, Loss: 0.0006271295715123415\n",
      "Batch: 284, Loss: 0.0024903209414333105\n",
      "Batch: 288, Loss: 0.0002131142100552097\n",
      "Batch: 292, Loss: 0.0036181278992444277\n",
      "Batch: 296, Loss: 0.011028582230210304\n",
      "Batch: 300, Loss: 0.0002242535847472027\n",
      "Batch: 304, Loss: 0.00023934085038490593\n",
      "Batch: 308, Loss: 0.001102881389670074\n",
      "Batch: 312, Loss: 0.003157542087137699\n",
      "Batch: 316, Loss: 0.001599981333129108\n",
      "Batch: 320, Loss: 2.9724978958256543e-05\n",
      "Batch: 324, Loss: 0.0002747639373410493\n",
      "Batch: 328, Loss: 0.0010182498954236507\n",
      "Batch: 332, Loss: 0.0013142881216481328\n",
      "Batch: 336, Loss: 0.01561792567372322\n",
      "Batch: 340, Loss: 0.00017748757090885192\n",
      "Batch: 344, Loss: 0.002882007509469986\n",
      "Batch: 348, Loss: 0.00014440692029893398\n",
      "Batch: 352, Loss: 0.005145484581589699\n",
      "Batch: 356, Loss: 0.003989980090409517\n",
      "Batch: 360, Loss: 0.00012792473717126995\n",
      "Batch: 364, Loss: 0.0009044933249242604\n",
      "Batch: 368, Loss: 0.0001671985228313133\n",
      "Batch: 372, Loss: 0.001682622474618256\n",
      "Batch: 376, Loss: 0.0006258541834540665\n",
      "Batch: 380, Loss: 3.116760854027234e-05\n",
      "Batch: 384, Loss: 0.000808906217571348\n",
      "Batch: 388, Loss: 0.0001052185325534083\n",
      "Batch: 392, Loss: 0.016514336690306664\n",
      "Batch: 396, Loss: 0.1121784970164299\n",
      "Batch: 400, Loss: 0.014958989806473255\n",
      "Batch: 404, Loss: 0.00020382122602313757\n",
      "Batch: 408, Loss: 0.018418950960040092\n",
      "Batch: 412, Loss: 0.025678712874650955\n",
      "Batch: 416, Loss: 0.0004497326444834471\n",
      "Batch: 420, Loss: 0.00020843632228206843\n",
      "Batch: 424, Loss: 0.003446643240749836\n",
      "Batch: 428, Loss: 0.0002900434483308345\n",
      "Batch: 432, Loss: 5.162494562682696e-05\n",
      "Batch: 436, Loss: 0.000516143103595823\n",
      "Batch: 440, Loss: 0.00022752537915948778\n",
      "Batch: 444, Loss: 0.0007741505978628993\n",
      "Batch: 448, Loss: 0.004540558438748121\n",
      "Batch: 452, Loss: 5.005585990147665e-05\n",
      "Batch: 456, Loss: 0.0013308365596458316\n",
      "Batch: 460, Loss: 0.06251361221075058\n",
      "Batch: 464, Loss: 3.8355152355507016e-05\n",
      "Batch: 468, Loss: 0.0029124203138053417\n",
      "Batch: 472, Loss: 0.00033916562097147107\n",
      "Batch: 476, Loss: 0.003333689644932747\n",
      "Batch: 480, Loss: 0.0016736926045268774\n",
      "Batch: 484, Loss: 8.861030801199377e-05\n",
      "Batch: 488, Loss: 0.0004284278256818652\n",
      "Batch: 492, Loss: 0.0009172541322186589\n",
      "Batch: 496, Loss: 0.001656772568821907\n",
      "Batch: 500, Loss: 0.005452816374599934\n",
      "Batch: 504, Loss: 0.0031569702550768852\n",
      "Batch: 508, Loss: 0.004810497630387545\n",
      "Batch: 512, Loss: 0.08697061240673065\n",
      "Batch: 516, Loss: 0.04473976790904999\n",
      "Batch: 520, Loss: 0.0008475292706862092\n",
      "Batch: 524, Loss: 0.0034284356515854597\n",
      "Batch: 528, Loss: 0.0002936738892458379\n",
      "Batch: 532, Loss: 0.005101597867906094\n",
      "Batch: 536, Loss: 3.891746382578276e-05\n",
      "Batch: 540, Loss: 7.646698941243812e-05\n",
      "Batch: 544, Loss: 0.002398058073595166\n",
      "Batch: 548, Loss: 0.0024413818027824163\n",
      "Batch: 552, Loss: 0.00010726587061071768\n",
      "Batch: 556, Loss: 0.0002660040627233684\n",
      "Batch: 560, Loss: 0.023753883317112923\n",
      "Batch: 564, Loss: 0.0028295947704464197\n",
      "Batch: 568, Loss: 0.004464814905077219\n",
      "Batch: 572, Loss: 0.019507689401507378\n",
      "Batch: 576, Loss: 0.001329713617451489\n",
      "Batch: 580, Loss: 0.0015015642857179046\n",
      "Batch: 584, Loss: 1.731717748043593e-05\n",
      "Batch: 588, Loss: 0.000233520389883779\n",
      "Batch: 592, Loss: 0.001043850090354681\n",
      "Batch: 596, Loss: 0.03866034001111984\n",
      "Batch: 600, Loss: 0.0538305826485157\n",
      "Batch: 604, Loss: 0.001288637169636786\n",
      "Batch: 608, Loss: 0.0016961231594905257\n",
      "Batch: 612, Loss: 0.04026651009917259\n",
      "Batch: 616, Loss: 2.112802758347243e-05\n",
      "Batch: 620, Loss: 0.0024152786936610937\n",
      "Batch: 624, Loss: 8.988523040898144e-05\n",
      "Batch: 628, Loss: 0.008336606435477734\n",
      "Batch: 632, Loss: 8.969721238827333e-05\n",
      "Batch: 636, Loss: 0.0023496414069086313\n",
      "Batch: 640, Loss: 0.009268032386898994\n",
      "Batch: 644, Loss: 0.002850098069757223\n",
      "Batch: 648, Loss: 0.011041237972676754\n",
      "Batch: 652, Loss: 0.01572280563414097\n",
      "Batch: 656, Loss: 0.0002578183193691075\n",
      "Batch: 660, Loss: 0.0023039360530674458\n",
      "Batch: 664, Loss: 0.00024813305935822427\n",
      "Batch: 668, Loss: 0.07714686542749405\n",
      "Batch: 672, Loss: 0.00011133878433611244\n",
      "Batch: 676, Loss: 0.000989890773780644\n",
      "Batch: 680, Loss: 0.0007585766725242138\n",
      "Batch: 684, Loss: 0.001043753232806921\n",
      "Batch: 688, Loss: 0.05393220856785774\n",
      "Batch: 692, Loss: 0.08864065259695053\n",
      "Batch: 696, Loss: 0.00036278762854635715\n",
      "Batch: 700, Loss: 0.0118726696819067\n",
      "Batch: 704, Loss: 0.015190355479717255\n",
      "Batch: 708, Loss: 0.04162881523370743\n",
      "Batch: 712, Loss: 0.00039659207686781883\n",
      "Batch: 716, Loss: 0.000283759378362447\n",
      "Batch: 720, Loss: 0.002230903599411249\n",
      "Batch: 724, Loss: 0.025163650512695312\n",
      "Batch: 728, Loss: 0.000615783385001123\n",
      "Batch: 732, Loss: 4.0796585381031036e-05\n",
      "Batch: 736, Loss: 0.010979319922626019\n",
      "Batch: 740, Loss: 0.00028860007296316326\n",
      "Batch: 744, Loss: 0.01283432450145483\n",
      "Batch: 748, Loss: 0.0004105731495656073\n",
      "Batch: 752, Loss: 0.0025812373496592045\n",
      "Batch: 756, Loss: 0.01082897000014782\n",
      "Batch: 760, Loss: 0.005707003176212311\n",
      "Batch: 764, Loss: 0.1044832170009613\n",
      "Batch: 768, Loss: 0.0004889353876933455\n",
      "Batch: 772, Loss: 0.0062151565216481686\n",
      "Batch: 776, Loss: 0.0044959476217627525\n",
      "Batch: 780, Loss: 0.0009348362218588591\n",
      "Batch: 784, Loss: 0.07009115070104599\n",
      "Batch: 788, Loss: 0.002459087874740362\n",
      "Batch: 792, Loss: 0.04622606560587883\n",
      "Batch: 796, Loss: 0.19823279976844788\n",
      "Batch: 800, Loss: 0.00029516650829464197\n",
      "Batch: 804, Loss: 0.027642350643873215\n",
      "Batch: 808, Loss: 0.048013731837272644\n",
      "Batch: 812, Loss: 0.00040920000174082816\n",
      "Batch: 816, Loss: 0.06810613721609116\n",
      "Batch: 820, Loss: 0.08601056784391403\n",
      "Batch: 824, Loss: 8.982733561424538e-06\n",
      "Batch: 828, Loss: 0.0007662390125915408\n",
      "Batch: 832, Loss: 0.002427843865007162\n",
      "Batch: 836, Loss: 0.00909827183932066\n",
      "Batch: 840, Loss: 0.00015415299276355654\n",
      "Batch: 844, Loss: 0.0012926761992275715\n",
      "Batch: 848, Loss: 0.003065308090299368\n",
      "Batch: 852, Loss: 0.018295569345355034\n",
      "Batch: 856, Loss: 0.002157781273126602\n",
      "Batch: 860, Loss: 0.008336621336638927\n",
      "Batch: 864, Loss: 0.0013482170179486275\n",
      "Batch: 868, Loss: 0.0006852131919004023\n",
      "Batch: 872, Loss: 0.05236508324742317\n",
      "Batch: 876, Loss: 0.05115252360701561\n",
      "Batch: 880, Loss: 0.0023620170541107655\n",
      "Batch: 884, Loss: 0.0071152509190142155\n",
      "Batch: 888, Loss: 0.0018863946897909045\n",
      "Batch: 892, Loss: 0.01739697903394699\n",
      "Batch: 896, Loss: 0.019958019256591797\n",
      "Batch: 900, Loss: 8.74034158186987e-05\n",
      "Batch: 904, Loss: 0.0006813809741288424\n",
      "Batch: 908, Loss: 0.0031895614229142666\n",
      "Batch: 912, Loss: 0.00014388238196261227\n",
      "Batch: 916, Loss: 0.0014466408174484968\n",
      "Batch: 920, Loss: 0.24273088574409485\n",
      "Batch: 924, Loss: 0.043665651232004166\n",
      "Batch: 928, Loss: 0.03728828951716423\n",
      "Batch: 932, Loss: 0.0031279432587325573\n",
      "Batch: 936, Loss: 0.054315511137247086\n",
      "\n",
      "验证集: Average loss: 0.0510, Accuracy: 9846/10000 (98%)\n",
      "\n",
      "Model weights saved to logs/model_weights_epoch0.pth\n",
      "Epoch 2\n",
      "Batch: 0, Loss: 0.0008431278401985765\n",
      "Batch: 4, Loss: 0.00683114817366004\n",
      "Batch: 8, Loss: 0.002024716231971979\n",
      "Batch: 12, Loss: 0.020168205723166466\n",
      "Batch: 16, Loss: 0.07198500633239746\n",
      "Batch: 20, Loss: 0.003591971006244421\n",
      "Batch: 24, Loss: 0.0005751995486207306\n",
      "Batch: 28, Loss: 0.0006220268551260233\n",
      "Batch: 32, Loss: 0.00023517449153587222\n",
      "Batch: 36, Loss: 0.017049266025424004\n",
      "Batch: 40, Loss: 0.00014148280024528503\n",
      "Batch: 44, Loss: 0.016230246052145958\n",
      "Batch: 48, Loss: 0.002339438535273075\n",
      "Batch: 52, Loss: 0.03893285244703293\n",
      "Batch: 56, Loss: 0.003814541734755039\n",
      "Batch: 60, Loss: 0.0009319630335085094\n",
      "Batch: 64, Loss: 0.004151554778218269\n",
      "Batch: 68, Loss: 0.0025581438094377518\n",
      "Batch: 72, Loss: 0.0017809817800298333\n",
      "Batch: 76, Loss: 0.0027172910049557686\n",
      "Batch: 80, Loss: 0.000989142688922584\n",
      "Batch: 84, Loss: 0.00011300954793114215\n",
      "Batch: 88, Loss: 0.0012991470284759998\n",
      "Batch: 92, Loss: 0.00018598356109578162\n",
      "Batch: 96, Loss: 0.000618522462900728\n",
      "Batch: 100, Loss: 3.6679710319731385e-05\n",
      "Batch: 104, Loss: 0.00025364538305439055\n",
      "Batch: 108, Loss: 0.00040764512959867716\n",
      "Batch: 112, Loss: 0.0003796179953496903\n",
      "Batch: 116, Loss: 0.000270263000857085\n",
      "Batch: 120, Loss: 2.483291973476298e-05\n",
      "Batch: 124, Loss: 0.02537277340888977\n",
      "Batch: 128, Loss: 0.016961868852376938\n",
      "Batch: 132, Loss: 0.00012296371278353035\n",
      "Batch: 136, Loss: 0.00016324354510288686\n",
      "Batch: 140, Loss: 0.0015776524087414145\n",
      "Batch: 144, Loss: 0.0011448418954387307\n",
      "Batch: 148, Loss: 0.021770792081952095\n",
      "Batch: 152, Loss: 0.00022794291726313531\n",
      "Batch: 156, Loss: 0.00016194352065213025\n",
      "Batch: 160, Loss: 0.00012511720706243068\n",
      "Batch: 164, Loss: 0.001909542246721685\n",
      "Batch: 168, Loss: 4.2588151700329036e-05\n",
      "Batch: 172, Loss: 0.0011130308266729116\n",
      "Batch: 176, Loss: 3.2565883884672076e-05\n",
      "Batch: 180, Loss: 0.001223899656906724\n",
      "Batch: 184, Loss: 7.017452389845857e-06\n",
      "Batch: 188, Loss: 5.847696229466237e-05\n",
      "Batch: 192, Loss: 0.0007215490913949907\n",
      "Batch: 196, Loss: 0.0002736558672040701\n",
      "Batch: 200, Loss: 0.0004170816100668162\n",
      "Batch: 204, Loss: 4.659075784729794e-05\n",
      "Batch: 208, Loss: 0.028935376554727554\n",
      "Batch: 212, Loss: 0.0011624378385022283\n",
      "Batch: 216, Loss: 4.113040995434858e-05\n",
      "Batch: 220, Loss: 0.00018625392112880945\n",
      "Batch: 224, Loss: 0.00035722527536563575\n",
      "Batch: 228, Loss: 0.00022224916028790176\n",
      "Batch: 232, Loss: 0.0006301681278273463\n",
      "Batch: 236, Loss: 0.0009943158365786076\n",
      "Batch: 240, Loss: 0.000335103104589507\n",
      "Batch: 244, Loss: 0.0006897867424413562\n",
      "Batch: 248, Loss: 0.00121846585534513\n",
      "Batch: 252, Loss: 0.002937399549409747\n",
      "Batch: 256, Loss: 0.0007208535098470747\n",
      "Batch: 260, Loss: 0.04927786812186241\n",
      "Batch: 264, Loss: 0.023599611595273018\n",
      "Batch: 268, Loss: 0.001589794410392642\n",
      "Batch: 272, Loss: 0.0036627482622861862\n",
      "Batch: 276, Loss: 0.0024064232129603624\n",
      "Batch: 280, Loss: 0.013056105002760887\n",
      "Batch: 284, Loss: 0.00026826353860087693\n",
      "Batch: 288, Loss: 9.32872062548995e-05\n",
      "Batch: 292, Loss: 7.89868863648735e-05\n",
      "Batch: 296, Loss: 0.0054152486845850945\n",
      "Batch: 300, Loss: 7.775327685521916e-05\n",
      "Batch: 304, Loss: 0.0002837666543200612\n",
      "Batch: 308, Loss: 0.0039839730598032475\n",
      "Batch: 312, Loss: 7.665418524993584e-05\n",
      "Batch: 316, Loss: 0.0007583793485537171\n",
      "Batch: 320, Loss: 0.0007438606698997319\n",
      "Batch: 324, Loss: 0.0006726117571815848\n",
      "Batch: 328, Loss: 0.00039964166353456676\n",
      "Batch: 332, Loss: 8.220925519708544e-05\n",
      "Batch: 336, Loss: 5.4499294492416084e-05\n",
      "Batch: 340, Loss: 0.00035053404280915856\n",
      "Batch: 344, Loss: 3.606950122048147e-05\n",
      "Batch: 348, Loss: 0.0038124574348330498\n",
      "Batch: 352, Loss: 0.0003731496399268508\n",
      "Batch: 356, Loss: 8.932083437684923e-05\n",
      "Batch: 360, Loss: 0.012327227741479874\n",
      "Batch: 364, Loss: 0.0013597554061561823\n",
      "Batch: 368, Loss: 0.00033385076676495373\n",
      "Batch: 372, Loss: 0.0026977809611707926\n",
      "Batch: 376, Loss: 0.089487224817276\n",
      "Batch: 380, Loss: 0.00021612364798784256\n",
      "Batch: 384, Loss: 0.0006178801413625479\n",
      "Batch: 388, Loss: 0.04486941173672676\n",
      "Batch: 392, Loss: 0.035430289804935455\n",
      "Batch: 396, Loss: 0.00021335647033993155\n",
      "Batch: 400, Loss: 0.002758176764473319\n",
      "Batch: 404, Loss: 0.00043872627429664135\n",
      "Batch: 408, Loss: 0.02437421679496765\n",
      "Batch: 412, Loss: 0.025229956954717636\n",
      "Batch: 416, Loss: 0.00017192421364597976\n",
      "Batch: 420, Loss: 0.0002907817834056914\n",
      "Batch: 424, Loss: 0.09667345136404037\n",
      "Batch: 428, Loss: 0.00017359733465127647\n",
      "Batch: 432, Loss: 0.017152905464172363\n",
      "Batch: 436, Loss: 0.000547641480807215\n",
      "Batch: 440, Loss: 0.0006762066623196006\n",
      "Batch: 444, Loss: 0.01869162544608116\n",
      "Batch: 448, Loss: 0.013788692653179169\n",
      "Batch: 452, Loss: 0.0010711534414440393\n",
      "Batch: 456, Loss: 0.00013780473091173917\n",
      "Batch: 460, Loss: 0.0325542576611042\n",
      "Batch: 464, Loss: 0.00029929791344329715\n",
      "Batch: 468, Loss: 0.0026283536572009325\n",
      "Batch: 472, Loss: 0.0005888387677259743\n",
      "Batch: 476, Loss: 0.019475657492876053\n",
      "Batch: 480, Loss: 2.6061352400574833e-05\n",
      "Batch: 484, Loss: 0.002048930386081338\n",
      "Batch: 488, Loss: 0.030023397877812386\n",
      "Batch: 492, Loss: 0.025149555876851082\n",
      "Batch: 496, Loss: 0.011967776343226433\n",
      "Batch: 500, Loss: 0.0015485535841435194\n",
      "Batch: 504, Loss: 0.05008827522397041\n",
      "Batch: 508, Loss: 3.1568815757054836e-05\n",
      "Batch: 512, Loss: 0.0002144155150745064\n",
      "Batch: 516, Loss: 0.00035005572135560215\n",
      "Batch: 520, Loss: 0.002425304614007473\n",
      "Batch: 524, Loss: 0.0016286110039800406\n",
      "Batch: 528, Loss: 0.03310677781701088\n",
      "Batch: 532, Loss: 0.0010243209544569254\n",
      "Batch: 536, Loss: 0.007734096609055996\n",
      "Batch: 540, Loss: 0.006744335405528545\n",
      "Batch: 544, Loss: 0.05899045243859291\n",
      "Batch: 548, Loss: 0.0010158829391002655\n",
      "Batch: 552, Loss: 0.002106733387336135\n",
      "Batch: 556, Loss: 0.0005792673910036683\n",
      "Batch: 560, Loss: 0.0003083808987867087\n",
      "Batch: 564, Loss: 0.0010844956850633025\n",
      "Batch: 568, Loss: 0.005350057501345873\n",
      "Batch: 572, Loss: 0.00023498789232689887\n",
      "Batch: 576, Loss: 0.0008142599835991859\n",
      "Batch: 580, Loss: 0.0014307948295027018\n",
      "Batch: 584, Loss: 0.009301870130002499\n",
      "Batch: 588, Loss: 0.00027499310090206563\n",
      "Batch: 592, Loss: 0.00027067644987255335\n",
      "Batch: 596, Loss: 0.008424662984907627\n",
      "Batch: 600, Loss: 3.3776792406570166e-05\n",
      "Batch: 604, Loss: 0.0004777953145094216\n",
      "Batch: 608, Loss: 0.000300041661830619\n",
      "Batch: 612, Loss: 0.0013341177254915237\n",
      "Batch: 616, Loss: 0.0008384245447814465\n",
      "Batch: 620, Loss: 0.005641220137476921\n",
      "Batch: 624, Loss: 1.0263907824992202e-05\n",
      "Batch: 628, Loss: 0.0016569803701713681\n",
      "Batch: 632, Loss: 1.9639275706140324e-05\n",
      "Batch: 636, Loss: 0.0015101202297955751\n",
      "Batch: 640, Loss: 0.008059236221015453\n",
      "Batch: 644, Loss: 0.0023380096536129713\n",
      "Batch: 648, Loss: 1.781749233487062e-05\n",
      "Batch: 652, Loss: 2.830729317793157e-05\n",
      "Batch: 656, Loss: 0.004024164285510778\n",
      "Batch: 660, Loss: 0.0023120290134102106\n",
      "Batch: 664, Loss: 1.6305337339872494e-05\n",
      "Batch: 668, Loss: 8.382511441595852e-05\n",
      "Batch: 672, Loss: 0.0004905328387394547\n",
      "Batch: 676, Loss: 0.00034006498754024506\n",
      "Batch: 680, Loss: 1.5594709111610427e-05\n",
      "Batch: 684, Loss: 2.5336801627418026e-05\n",
      "Batch: 688, Loss: 0.0010157594224438071\n",
      "Batch: 692, Loss: 3.1528350518783554e-05\n",
      "Batch: 696, Loss: 3.186518733855337e-05\n",
      "Batch: 700, Loss: 0.00014542302233166993\n",
      "Batch: 704, Loss: 0.013088730163872242\n",
      "Batch: 708, Loss: 0.009840823709964752\n",
      "Batch: 712, Loss: 3.68657274520956e-05\n",
      "Batch: 716, Loss: 0.0010140545200556517\n",
      "Batch: 720, Loss: 0.001284679165109992\n",
      "Batch: 724, Loss: 0.0030345763079822063\n",
      "Batch: 728, Loss: 0.0014303772477433085\n",
      "Batch: 732, Loss: 6.988514360273257e-05\n",
      "Batch: 736, Loss: 0.0004629466275218874\n",
      "Batch: 740, Loss: 6.092420517234132e-05\n",
      "Batch: 744, Loss: 0.004314159508794546\n",
      "Batch: 748, Loss: 0.00122850073967129\n",
      "Batch: 752, Loss: 0.03737920895218849\n",
      "Batch: 756, Loss: 0.0003025451151188463\n",
      "Batch: 760, Loss: 0.10633334517478943\n",
      "Batch: 764, Loss: 0.06017138808965683\n",
      "Batch: 768, Loss: 0.00012505710765253752\n",
      "Batch: 772, Loss: 0.00243410374969244\n",
      "Batch: 776, Loss: 0.017160505056381226\n",
      "Batch: 780, Loss: 0.0032309512607753277\n",
      "Batch: 784, Loss: 0.00018505120533518493\n",
      "Batch: 788, Loss: 0.0013624019920825958\n",
      "Batch: 792, Loss: 0.0004232452774886042\n",
      "Batch: 796, Loss: 0.02175886742770672\n",
      "Batch: 800, Loss: 0.00014835922047495842\n",
      "Batch: 804, Loss: 0.00017811954603530467\n",
      "Batch: 808, Loss: 0.0022266351152211428\n",
      "Batch: 812, Loss: 0.0012976267607882619\n",
      "Batch: 816, Loss: 0.08771039545536041\n",
      "Batch: 820, Loss: 0.00013734583626501262\n",
      "Batch: 824, Loss: 0.0002635058481246233\n",
      "Batch: 828, Loss: 0.034205999225378036\n",
      "Batch: 832, Loss: 0.021001992747187614\n",
      "Batch: 836, Loss: 0.0003898909199051559\n",
      "Batch: 840, Loss: 0.0008004740229807794\n",
      "Batch: 844, Loss: 0.0016018152236938477\n",
      "Batch: 848, Loss: 5.5761665862519294e-05\n",
      "Batch: 852, Loss: 6.79934091749601e-05\n",
      "Batch: 856, Loss: 0.00022814481053501368\n",
      "Batch: 860, Loss: 2.973392474814318e-05\n",
      "Batch: 864, Loss: 0.11780254542827606\n",
      "Batch: 868, Loss: 2.959557605208829e-05\n",
      "Batch: 872, Loss: 0.004198539070785046\n",
      "Batch: 876, Loss: 0.004801290575414896\n",
      "Batch: 880, Loss: 0.0209809597581625\n",
      "Batch: 884, Loss: 0.0007498710765503347\n",
      "Batch: 888, Loss: 0.06541628390550613\n",
      "Batch: 892, Loss: 9.7725132945925e-06\n",
      "Batch: 896, Loss: 0.0003537532757036388\n",
      "Batch: 900, Loss: 3.65767082257662e-05\n",
      "Batch: 904, Loss: 4.422260099090636e-05\n",
      "Batch: 908, Loss: 0.00020677699649240822\n",
      "Batch: 912, Loss: 0.001065024291165173\n",
      "Batch: 916, Loss: 0.003297257237136364\n",
      "Batch: 920, Loss: 0.020529521629214287\n",
      "Batch: 924, Loss: 0.000523075636010617\n",
      "Batch: 928, Loss: 0.004680246580392122\n",
      "Batch: 932, Loss: 5.2642630180343986e-05\n",
      "Batch: 936, Loss: 0.0026237734127789736\n",
      "\n",
      "验证集: Average loss: 0.0390, Accuracy: 9902/10000 (99%)\n",
      "\n",
      "Model weights saved to logs/model_weights_epoch1.pth\n",
      "Epoch 3\n",
      "Batch: 0, Loss: 0.0001581649703439325\n",
      "Batch: 4, Loss: 0.0005065222503617406\n",
      "Batch: 8, Loss: 8.66030495672021e-06\n",
      "Batch: 12, Loss: 0.00014957757957745343\n",
      "Batch: 16, Loss: 0.0037561377976089716\n",
      "Batch: 20, Loss: 0.023592079058289528\n",
      "Batch: 24, Loss: 8.355791942449287e-05\n",
      "Batch: 28, Loss: 2.2203594198799692e-05\n",
      "Batch: 32, Loss: 0.025669002905488014\n",
      "Batch: 36, Loss: 0.0039083585143089294\n",
      "Batch: 40, Loss: 0.05928322672843933\n",
      "Batch: 44, Loss: 0.015302673913538456\n",
      "Batch: 48, Loss: 0.00013808475341647863\n",
      "Batch: 52, Loss: 2.099359335261397e-05\n",
      "Batch: 56, Loss: 0.0002301406639162451\n",
      "Batch: 60, Loss: 0.00042165545164607465\n",
      "Batch: 64, Loss: 0.011648770421743393\n",
      "Batch: 68, Loss: 0.03552090376615524\n",
      "Batch: 72, Loss: 0.0003250393783673644\n",
      "Batch: 76, Loss: 0.004180700518190861\n",
      "Batch: 80, Loss: 6.653963646385819e-05\n",
      "Batch: 84, Loss: 0.0006409972556866705\n",
      "Batch: 88, Loss: 1.2894024621346034e-05\n",
      "Batch: 92, Loss: 0.02640571817755699\n",
      "Batch: 96, Loss: 0.023303069174289703\n",
      "Batch: 100, Loss: 0.013399648480117321\n",
      "Batch: 104, Loss: 0.00014285197539720684\n",
      "Batch: 108, Loss: 0.0012238643830642104\n",
      "Batch: 112, Loss: 0.053129710257053375\n",
      "Batch: 116, Loss: 5.617807255475782e-05\n",
      "Batch: 120, Loss: 0.0009146169759333134\n",
      "Batch: 124, Loss: 0.013977941125631332\n",
      "Batch: 128, Loss: 6.165802187751979e-05\n",
      "Batch: 132, Loss: 0.00519097363576293\n",
      "Batch: 136, Loss: 0.0015490086516365409\n",
      "Batch: 140, Loss: 0.0006889935466460884\n",
      "Batch: 144, Loss: 0.00020505799329839647\n",
      "Batch: 148, Loss: 0.00116351165343076\n",
      "Batch: 152, Loss: 0.0008259001187980175\n",
      "Batch: 156, Loss: 0.00029795311274938285\n",
      "Batch: 160, Loss: 0.00020199900609441102\n",
      "Batch: 164, Loss: 0.01072729006409645\n",
      "Batch: 168, Loss: 0.002422469202429056\n",
      "Batch: 172, Loss: 0.00043235765770077705\n",
      "Batch: 176, Loss: 9.798770406632684e-06\n",
      "Batch: 180, Loss: 0.00018810003530234098\n",
      "Batch: 184, Loss: 0.0006372457719407976\n",
      "Batch: 188, Loss: 0.0010240350384265184\n",
      "Batch: 192, Loss: 9.380402480019256e-05\n",
      "Batch: 196, Loss: 0.010761088691651821\n",
      "Batch: 200, Loss: 0.04014895483851433\n",
      "Batch: 204, Loss: 8.440880264970474e-06\n",
      "Batch: 208, Loss: 0.002524474635720253\n",
      "Batch: 212, Loss: 7.948993152240291e-05\n",
      "Batch: 216, Loss: 7.150160672608763e-05\n",
      "Batch: 220, Loss: 0.012147806584835052\n",
      "Batch: 224, Loss: 0.029493756592273712\n",
      "Batch: 228, Loss: 0.0010937745682895184\n",
      "Batch: 232, Loss: 0.0003533166600391269\n",
      "Batch: 236, Loss: 5.66774087928934e-06\n",
      "Batch: 240, Loss: 4.171847649558913e-06\n",
      "Batch: 244, Loss: 0.0007260694983415306\n",
      "Batch: 248, Loss: 4.430964509083424e-06\n",
      "Batch: 252, Loss: 2.4947210476966575e-05\n",
      "Batch: 256, Loss: 0.00022822430764790624\n",
      "Batch: 260, Loss: 0.0019961707293987274\n",
      "Batch: 264, Loss: 0.0005313142319209874\n",
      "Batch: 268, Loss: 0.00023369975679088384\n",
      "Batch: 272, Loss: 3.015304173459299e-05\n",
      "Batch: 276, Loss: 0.03530867025256157\n",
      "Batch: 280, Loss: 0.00010367902723373845\n",
      "Batch: 284, Loss: 0.00019556340703275055\n",
      "Batch: 288, Loss: 0.0027870163321495056\n",
      "Batch: 292, Loss: 7.366311911027879e-05\n",
      "Batch: 296, Loss: 0.0032236017286777496\n",
      "Batch: 300, Loss: 0.00015843971050344408\n",
      "Batch: 304, Loss: 0.0003460849402472377\n",
      "Batch: 308, Loss: 0.0009003177401609719\n",
      "Batch: 312, Loss: 9.894170216284692e-05\n",
      "Batch: 316, Loss: 0.0006987368687987328\n",
      "Batch: 320, Loss: 0.0017479530069977045\n",
      "Batch: 324, Loss: 5.870507720828755e-06\n",
      "Batch: 328, Loss: 1.2739280464302283e-05\n",
      "Batch: 332, Loss: 0.0003133511054329574\n",
      "Batch: 336, Loss: 0.0022268244065344334\n",
      "Batch: 340, Loss: 2.4912978915381245e-05\n",
      "Batch: 344, Loss: 0.07994863390922546\n",
      "Batch: 348, Loss: 0.003986070863902569\n",
      "Batch: 352, Loss: 0.00397464819252491\n",
      "Batch: 356, Loss: 3.183117223670706e-05\n",
      "Batch: 360, Loss: 0.013789159245789051\n",
      "Batch: 364, Loss: 0.0024727133568376303\n",
      "Batch: 368, Loss: 0.00010219146497547626\n",
      "Batch: 372, Loss: 0.00010315916733816266\n",
      "Batch: 376, Loss: 0.0023661863524466753\n",
      "Batch: 380, Loss: 0.0009335589129477739\n",
      "Batch: 384, Loss: 0.0017799092456698418\n",
      "Batch: 388, Loss: 4.0113496652338654e-05\n",
      "Batch: 392, Loss: 0.0006879611173644662\n",
      "Batch: 396, Loss: 0.00012614562001544982\n",
      "Batch: 400, Loss: 0.00029397723847068846\n",
      "Batch: 404, Loss: 0.003498987527564168\n",
      "Batch: 408, Loss: 0.06842097640037537\n",
      "Batch: 412, Loss: 0.0013721734285354614\n",
      "Batch: 416, Loss: 9.636158938519657e-05\n",
      "Batch: 420, Loss: 0.00045825354754924774\n",
      "Batch: 424, Loss: 6.7443252191878855e-06\n",
      "Batch: 428, Loss: 0.0001676055253483355\n",
      "Batch: 432, Loss: 0.0006372460629791021\n",
      "Batch: 436, Loss: 0.00012830075866077095\n",
      "Batch: 440, Loss: 0.0005740487249568105\n",
      "Batch: 444, Loss: 0.00010134692274732515\n",
      "Batch: 448, Loss: 0.001741334330290556\n",
      "Batch: 452, Loss: 0.0028681494295597076\n",
      "Batch: 456, Loss: 8.087717105809133e-06\n",
      "Batch: 460, Loss: 0.004224459175020456\n",
      "Batch: 464, Loss: 0.0051348209381103516\n",
      "Batch: 468, Loss: 0.026976002380251884\n",
      "Batch: 472, Loss: 0.03363880142569542\n",
      "Batch: 476, Loss: 0.023962415754795074\n",
      "Batch: 480, Loss: 0.00011292865383438766\n",
      "Batch: 484, Loss: 0.005777674727141857\n",
      "Batch: 488, Loss: 1.3486974239640404e-05\n",
      "Batch: 492, Loss: 7.770411320962012e-05\n",
      "Batch: 496, Loss: 0.00016317135305143893\n",
      "Batch: 500, Loss: 0.000950014975387603\n",
      "Batch: 504, Loss: 4.287523915991187e-05\n",
      "Batch: 508, Loss: 3.291859320597723e-05\n",
      "Batch: 512, Loss: 0.009627901948988438\n",
      "Batch: 516, Loss: 0.00021149709937162697\n",
      "Batch: 520, Loss: 4.046448157168925e-05\n",
      "Batch: 524, Loss: 0.005797445774078369\n",
      "Batch: 528, Loss: 0.003922765143215656\n",
      "Batch: 532, Loss: 1.8478189304005355e-05\n",
      "Batch: 536, Loss: 0.009007774293422699\n",
      "Batch: 540, Loss: 3.3752225135685876e-05\n",
      "Batch: 544, Loss: 0.0013689317274838686\n",
      "Batch: 548, Loss: 0.0003404996241442859\n",
      "Batch: 552, Loss: 0.0009205547976307571\n",
      "Batch: 556, Loss: 0.002959032077342272\n",
      "Batch: 560, Loss: 5.0872644351329654e-05\n",
      "Batch: 564, Loss: 0.00011891889153048396\n",
      "Batch: 568, Loss: 0.002863239496946335\n",
      "Batch: 572, Loss: 0.0055360631085932255\n",
      "Batch: 576, Loss: 1.9572686142055318e-05\n",
      "Batch: 580, Loss: 0.06548549234867096\n",
      "Batch: 584, Loss: 8.590039942646399e-05\n",
      "Batch: 588, Loss: 0.0002595289552118629\n",
      "Batch: 592, Loss: 0.007647122722119093\n",
      "Batch: 596, Loss: 0.07432733476161957\n",
      "Batch: 600, Loss: 3.3091826480813324e-05\n",
      "Batch: 604, Loss: 0.00010461890633450821\n",
      "Batch: 608, Loss: 7.934369205031544e-05\n",
      "Batch: 612, Loss: 3.297911462141201e-05\n",
      "Batch: 616, Loss: 0.004152259323745966\n",
      "Batch: 620, Loss: 1.5921030353638344e-05\n",
      "Batch: 624, Loss: 0.0001640668197069317\n",
      "Batch: 628, Loss: 0.00035120657412335277\n",
      "Batch: 632, Loss: 0.0023694350384175777\n",
      "Batch: 636, Loss: 0.0044021871872246265\n",
      "Batch: 640, Loss: 0.00010557923087617382\n",
      "Batch: 644, Loss: 7.028969412203878e-05\n",
      "Batch: 648, Loss: 0.00023347030219156295\n",
      "Batch: 652, Loss: 0.005276711657643318\n",
      "Batch: 656, Loss: 2.2283606085693464e-05\n",
      "Batch: 660, Loss: 1.4241973076423164e-05\n",
      "Batch: 664, Loss: 0.0013850515242666006\n",
      "Batch: 668, Loss: 0.0001479803177062422\n",
      "Batch: 672, Loss: 0.0007888930267654359\n",
      "Batch: 676, Loss: 6.038343781256117e-05\n",
      "Batch: 680, Loss: 0.06174033507704735\n",
      "Batch: 684, Loss: 0.040924493223428726\n",
      "Batch: 688, Loss: 0.0022657078225165606\n",
      "Batch: 692, Loss: 0.02036193571984768\n",
      "Batch: 696, Loss: 0.015451142564415932\n",
      "Batch: 700, Loss: 0.08423448354005814\n",
      "Batch: 704, Loss: 0.004236867651343346\n",
      "Batch: 708, Loss: 0.00011021268437616527\n",
      "Batch: 712, Loss: 0.002784671960398555\n",
      "Batch: 716, Loss: 0.013465830124914646\n",
      "Batch: 720, Loss: 0.0003818145196419209\n",
      "Batch: 724, Loss: 0.00022707214520778507\n",
      "Batch: 728, Loss: 0.05114278197288513\n",
      "Batch: 732, Loss: 0.00036003527930006385\n",
      "Batch: 736, Loss: 0.019487252458930016\n",
      "Batch: 740, Loss: 0.009078227914869785\n",
      "Batch: 744, Loss: 2.5245602955692448e-05\n",
      "Batch: 748, Loss: 0.0015459202695637941\n",
      "Batch: 752, Loss: 0.002138532465323806\n",
      "Batch: 756, Loss: 0.0002902447886299342\n",
      "Batch: 760, Loss: 6.961690814932808e-05\n",
      "Batch: 764, Loss: 0.00019541244546417147\n",
      "Batch: 768, Loss: 0.002953878603875637\n",
      "Batch: 772, Loss: 0.05427780747413635\n",
      "Batch: 776, Loss: 0.017962804064154625\n",
      "Batch: 780, Loss: 0.000834482372738421\n",
      "Batch: 784, Loss: 8.49198258947581e-05\n",
      "Batch: 788, Loss: 0.01994672790169716\n",
      "Batch: 792, Loss: 1.7585711248102598e-05\n",
      "Batch: 796, Loss: 0.0002684479404706508\n",
      "Batch: 800, Loss: 0.0009580260375514627\n",
      "Batch: 804, Loss: 0.0027372813783586025\n",
      "Batch: 808, Loss: 0.0016776013653725386\n",
      "Batch: 812, Loss: 0.05399811267852783\n",
      "Batch: 816, Loss: 0.0009353171335533261\n",
      "Batch: 820, Loss: 0.002800275105983019\n",
      "Batch: 824, Loss: 0.00021322706015780568\n",
      "Batch: 828, Loss: 0.00014348210243042558\n",
      "Batch: 832, Loss: 0.00013011624105274677\n",
      "Batch: 836, Loss: 0.001118992455303669\n",
      "Batch: 840, Loss: 0.00046720492537133396\n",
      "Batch: 844, Loss: 0.014308820478618145\n",
      "Batch: 848, Loss: 6.164964725030586e-05\n",
      "Batch: 852, Loss: 0.00017727304657455534\n",
      "Batch: 856, Loss: 0.052519481629133224\n",
      "Batch: 860, Loss: 0.031153544783592224\n",
      "Batch: 864, Loss: 0.00980773288756609\n",
      "Batch: 868, Loss: 0.0036879980470985174\n",
      "Batch: 872, Loss: 0.003557197516784072\n",
      "Batch: 876, Loss: 2.3154654627433047e-05\n",
      "Batch: 880, Loss: 0.02332211472094059\n",
      "Batch: 884, Loss: 0.10518360882997513\n",
      "Batch: 888, Loss: 0.0008705377695150673\n",
      "Batch: 892, Loss: 0.04046234115958214\n",
      "Batch: 896, Loss: 0.0008651235257275403\n",
      "Batch: 900, Loss: 0.0049324631690979\n",
      "Batch: 904, Loss: 0.00024778215447440743\n",
      "Batch: 908, Loss: 0.004969182424247265\n",
      "Batch: 912, Loss: 0.002426574472337961\n",
      "Batch: 916, Loss: 5.763035369454883e-05\n",
      "Batch: 920, Loss: 5.0973227189388126e-05\n",
      "Batch: 924, Loss: 0.001869615982286632\n",
      "Batch: 928, Loss: 0.06908765435218811\n",
      "Batch: 932, Loss: 8.912105113267899e-05\n",
      "Batch: 936, Loss: 0.019298261031508446\n",
      "\n",
      "验证集: Average loss: 0.0459, Accuracy: 9875/10000 (99%)\n",
      "\n",
      "Model weights saved to logs/model_weights_epoch2.pth\n",
      "Epoch 4\n",
      "Batch: 0, Loss: 0.00021306290000211447\n",
      "Batch: 4, Loss: 3.0892355425748974e-05\n",
      "Batch: 8, Loss: 0.001118105254136026\n",
      "Batch: 12, Loss: 0.003237007185816765\n",
      "Batch: 16, Loss: 2.423846672172658e-05\n",
      "Batch: 20, Loss: 0.0029292195104062557\n",
      "Batch: 24, Loss: 0.0003883893950842321\n",
      "Batch: 28, Loss: 2.9970124160172418e-05\n",
      "Batch: 32, Loss: 4.178304152446799e-05\n",
      "Batch: 36, Loss: 0.003258391050621867\n",
      "Batch: 40, Loss: 0.022555988281965256\n",
      "Batch: 44, Loss: 0.0001660394191276282\n",
      "Batch: 48, Loss: 0.005963487550616264\n",
      "Batch: 52, Loss: 0.0018742440734058619\n",
      "Batch: 56, Loss: 0.006441591307520866\n",
      "Batch: 60, Loss: 0.0006791165797039866\n",
      "Batch: 64, Loss: 0.0001772083342075348\n",
      "Batch: 68, Loss: 0.035073839128017426\n",
      "Batch: 72, Loss: 0.00011197618732694536\n",
      "Batch: 76, Loss: 0.002016103360801935\n",
      "Batch: 80, Loss: 0.0006911171949468553\n",
      "Batch: 84, Loss: 2.932165989477653e-05\n",
      "Batch: 88, Loss: 0.00018613685097079724\n",
      "Batch: 92, Loss: 0.0005207260837778449\n",
      "Batch: 96, Loss: 0.05797396972775459\n",
      "Batch: 100, Loss: 0.002346718218177557\n",
      "Batch: 104, Loss: 0.0008694257121533155\n",
      "Batch: 108, Loss: 0.0004994681803509593\n",
      "Batch: 112, Loss: 1.8945760530186817e-05\n",
      "Batch: 116, Loss: 6.616028258576989e-05\n",
      "Batch: 120, Loss: 0.000130024433019571\n",
      "Batch: 124, Loss: 3.529491004883312e-05\n",
      "Batch: 128, Loss: 0.00011492311023175716\n",
      "Batch: 132, Loss: 0.07298317551612854\n",
      "Batch: 136, Loss: 6.0100301197962835e-05\n",
      "Batch: 140, Loss: 0.00039004531572572887\n",
      "Batch: 144, Loss: 0.052198395133018494\n",
      "Batch: 148, Loss: 2.8701468181679957e-05\n",
      "Batch: 152, Loss: 0.003688669763505459\n",
      "Batch: 156, Loss: 0.008142329752445221\n",
      "Batch: 160, Loss: 0.0031705398578196764\n",
      "Batch: 164, Loss: 0.0007770555093884468\n",
      "Batch: 168, Loss: 0.0008619260042905807\n",
      "Batch: 172, Loss: 0.00014944108261261135\n",
      "Batch: 176, Loss: 0.0004274557577446103\n",
      "Batch: 180, Loss: 0.01015346311032772\n",
      "Batch: 184, Loss: 2.9484767765097786e-06\n",
      "Batch: 188, Loss: 0.010104306973516941\n",
      "Batch: 192, Loss: 5.403307113738265e-06\n",
      "Batch: 196, Loss: 3.285966158728115e-05\n",
      "Batch: 200, Loss: 0.006676918361335993\n",
      "Batch: 204, Loss: 0.0029326537624001503\n",
      "Batch: 208, Loss: 3.933332482120022e-05\n",
      "Batch: 212, Loss: 0.0004252135695423931\n",
      "Batch: 216, Loss: 0.0012412209762260318\n",
      "Batch: 220, Loss: 1.1276225450274069e-05\n",
      "Batch: 224, Loss: 5.808934020024026e-06\n",
      "Batch: 228, Loss: 0.011654738336801529\n",
      "Batch: 232, Loss: 0.00010023845243267715\n",
      "Batch: 236, Loss: 0.00036460888804867864\n",
      "Batch: 240, Loss: 0.00014144708984531462\n",
      "Batch: 244, Loss: 0.00030987084028311074\n",
      "Batch: 248, Loss: 1.6298730770358816e-05\n",
      "Batch: 252, Loss: 6.440486322389916e-05\n",
      "Batch: 256, Loss: 0.002024260349571705\n",
      "Batch: 260, Loss: 0.0017253365367650986\n",
      "Batch: 264, Loss: 2.1201540221227333e-05\n",
      "Batch: 268, Loss: 1.943275310622994e-05\n",
      "Batch: 272, Loss: 0.00014169620408210903\n",
      "Batch: 276, Loss: 0.00035538108204491436\n",
      "Batch: 280, Loss: 6.479153671534732e-05\n",
      "Batch: 284, Loss: 0.0009115612483583391\n",
      "Batch: 288, Loss: 0.0012041055597364902\n",
      "Batch: 292, Loss: 0.0015968871302902699\n",
      "Batch: 296, Loss: 9.198999759973958e-05\n",
      "Batch: 300, Loss: 0.0018299416406080127\n",
      "Batch: 304, Loss: 1.0989999282173812e-05\n",
      "Batch: 308, Loss: 0.0002784099488053471\n",
      "Batch: 312, Loss: 0.11158287525177002\n",
      "Batch: 316, Loss: 8.616327249910682e-05\n",
      "Batch: 320, Loss: 2.3140695702750236e-05\n",
      "Batch: 324, Loss: 0.004317312501370907\n",
      "Batch: 328, Loss: 0.0009853875963017344\n",
      "Batch: 332, Loss: 0.00047968621947802603\n",
      "Batch: 336, Loss: 3.11988287649001e-06\n",
      "Batch: 340, Loss: 2.9020156944170594e-05\n",
      "Batch: 344, Loss: 1.1969097613473423e-05\n",
      "Batch: 348, Loss: 0.00027434428920969367\n",
      "Batch: 352, Loss: 0.009229018352925777\n",
      "Batch: 356, Loss: 0.00010829897655639797\n",
      "Batch: 360, Loss: 0.0010960949584841728\n",
      "Batch: 364, Loss: 0.005115784704685211\n",
      "Batch: 368, Loss: 0.004053843207657337\n",
      "Batch: 372, Loss: 0.00018298678332939744\n",
      "Batch: 376, Loss: 0.005569439381361008\n",
      "Batch: 380, Loss: 0.0011781101347878575\n",
      "Batch: 384, Loss: 3.2668547191860853e-06\n",
      "Batch: 388, Loss: 0.00018066345364786685\n",
      "Batch: 392, Loss: 0.0008394309552386403\n",
      "Batch: 396, Loss: 3.4468866942916065e-05\n",
      "Batch: 400, Loss: 2.8794744139304385e-05\n",
      "Batch: 404, Loss: 0.00016842776676639915\n",
      "Batch: 408, Loss: 0.0002588503703009337\n",
      "Batch: 412, Loss: 0.002125455066561699\n",
      "Batch: 416, Loss: 0.005654531065374613\n",
      "Batch: 420, Loss: 3.5608049074653536e-05\n",
      "Batch: 424, Loss: 0.00033129635266959667\n",
      "Batch: 428, Loss: 4.581972461892292e-05\n",
      "Batch: 432, Loss: 0.0008712223498150706\n",
      "Batch: 436, Loss: 9.84067446552217e-05\n",
      "Batch: 440, Loss: 0.0004994472255930305\n",
      "Batch: 444, Loss: 0.000699036056175828\n",
      "Batch: 448, Loss: 0.002458592876791954\n",
      "Batch: 452, Loss: 3.186735193594359e-05\n",
      "Batch: 456, Loss: 2.6445773983141407e-05\n",
      "Batch: 460, Loss: 0.0008140965364873409\n",
      "Batch: 464, Loss: 0.00032040124642662704\n",
      "Batch: 468, Loss: 0.00020671141101047397\n",
      "Batch: 472, Loss: 2.851626504707383e-06\n",
      "Batch: 476, Loss: 0.04059687256813049\n",
      "Batch: 480, Loss: 0.0036498147528618574\n",
      "Batch: 484, Loss: 0.0005658955778926611\n",
      "Batch: 488, Loss: 0.0006582713685929775\n",
      "Batch: 492, Loss: 0.0010762708261609077\n",
      "Batch: 496, Loss: 0.0024976348504424095\n",
      "Batch: 500, Loss: 5.4434985941043124e-05\n",
      "Batch: 504, Loss: 0.010067972354590893\n",
      "Batch: 508, Loss: 0.01740150712430477\n",
      "Batch: 512, Loss: 0.010436640121042728\n",
      "Batch: 516, Loss: 3.901580203091726e-05\n",
      "Batch: 520, Loss: 0.0014230589149519801\n",
      "Batch: 524, Loss: 0.0033260078635066748\n",
      "Batch: 528, Loss: 0.016018986701965332\n",
      "Batch: 532, Loss: 0.020622877404093742\n",
      "Batch: 536, Loss: 0.017285073176026344\n",
      "Batch: 540, Loss: 0.012852747924625874\n",
      "Batch: 544, Loss: 0.19282342493534088\n",
      "Batch: 548, Loss: 0.015941912308335304\n",
      "Batch: 552, Loss: 5.268325185170397e-05\n",
      "Batch: 556, Loss: 0.0018211734713986516\n",
      "Batch: 560, Loss: 0.10965638607740402\n",
      "Batch: 564, Loss: 8.44437163323164e-05\n",
      "Batch: 568, Loss: 0.005345253739506006\n",
      "Batch: 572, Loss: 0.010248415172100067\n",
      "Batch: 576, Loss: 0.0007591202738694847\n",
      "Batch: 580, Loss: 7.014181846898282e-06\n",
      "Batch: 584, Loss: 2.030757059401367e-05\n",
      "Batch: 588, Loss: 7.728048512944952e-05\n",
      "Batch: 592, Loss: 4.10234606533777e-05\n",
      "Batch: 596, Loss: 0.02372003346681595\n",
      "Batch: 600, Loss: 0.0003360119881108403\n",
      "Batch: 604, Loss: 0.001962283393368125\n",
      "Batch: 608, Loss: 1.7813585145631805e-05\n",
      "Batch: 612, Loss: 0.0025672833435237408\n",
      "Batch: 616, Loss: 0.01932780258357525\n",
      "Batch: 620, Loss: 0.06040068343281746\n",
      "Batch: 624, Loss: 0.0017975736409425735\n",
      "Batch: 628, Loss: 1.8071674276143312e-05\n",
      "Batch: 632, Loss: 7.770510273985565e-05\n",
      "Batch: 636, Loss: 9.277981735067442e-05\n",
      "Batch: 640, Loss: 0.006618003360927105\n",
      "Batch: 644, Loss: 0.05170755833387375\n",
      "Batch: 648, Loss: 0.0002948541659861803\n",
      "Batch: 652, Loss: 0.04077399522066116\n",
      "Batch: 656, Loss: 0.011318102478981018\n",
      "Batch: 660, Loss: 4.3737167288782075e-05\n",
      "Batch: 664, Loss: 0.0001737373968353495\n",
      "Batch: 668, Loss: 0.07275371998548508\n",
      "Batch: 672, Loss: 0.0006877522682771087\n",
      "Batch: 676, Loss: 0.009809376671910286\n",
      "Batch: 680, Loss: 0.0390467569231987\n",
      "Batch: 684, Loss: 0.007317806594073772\n",
      "Batch: 688, Loss: 0.002032543532550335\n",
      "Batch: 692, Loss: 5.425516428658739e-05\n",
      "Batch: 696, Loss: 0.02087493985891342\n",
      "Batch: 700, Loss: 0.06733018159866333\n",
      "Batch: 704, Loss: 0.0003213101881556213\n",
      "Batch: 708, Loss: 0.0031805492471903563\n",
      "Batch: 712, Loss: 0.013107276521623135\n",
      "Batch: 716, Loss: 0.10580018162727356\n",
      "Batch: 720, Loss: 0.010521532036364079\n",
      "Batch: 724, Loss: 0.005374365486204624\n",
      "Batch: 728, Loss: 0.0028214778285473585\n",
      "Batch: 732, Loss: 0.07767904549837112\n",
      "Batch: 736, Loss: 0.0013040009653195739\n",
      "Batch: 740, Loss: 0.01873653382062912\n",
      "Batch: 744, Loss: 0.008226735517382622\n",
      "Batch: 748, Loss: 5.57470484636724e-05\n",
      "Batch: 752, Loss: 0.0023455116897821426\n",
      "Batch: 756, Loss: 0.03114204853773117\n",
      "Batch: 760, Loss: 0.027568241581320763\n",
      "Batch: 764, Loss: 0.00029173688380979\n",
      "Batch: 768, Loss: 4.2600800952641293e-05\n",
      "Batch: 772, Loss: 0.0030429211910814047\n",
      "Batch: 776, Loss: 0.002809299388900399\n",
      "Batch: 780, Loss: 0.06776774674654007\n",
      "Batch: 784, Loss: 0.003198075806722045\n",
      "Batch: 788, Loss: 0.0005098233232274652\n",
      "Batch: 792, Loss: 0.0004979833611287177\n",
      "Batch: 796, Loss: 0.01634727045893669\n",
      "Batch: 800, Loss: 0.0011671434622257948\n",
      "Batch: 804, Loss: 0.009503476321697235\n",
      "Batch: 808, Loss: 0.0017192355589941144\n",
      "Batch: 812, Loss: 0.0009621500503271818\n",
      "Batch: 816, Loss: 0.0069207120686769485\n",
      "Batch: 820, Loss: 0.000691068999003619\n",
      "Batch: 824, Loss: 0.0007729098433628678\n",
      "Batch: 828, Loss: 0.00010518670751480386\n",
      "Batch: 832, Loss: 5.930835322942585e-05\n",
      "Batch: 836, Loss: 0.0001107907883124426\n",
      "Batch: 840, Loss: 3.0835963116260245e-05\n",
      "Batch: 844, Loss: 0.004024754278361797\n",
      "Batch: 848, Loss: 0.0009772391058504581\n",
      "Batch: 852, Loss: 0.0811857134103775\n",
      "Batch: 856, Loss: 0.0013070479035377502\n",
      "Batch: 860, Loss: 0.0002564843453001231\n",
      "Batch: 864, Loss: 0.00020829291315749288\n",
      "Batch: 868, Loss: 0.00017239963926840574\n",
      "Batch: 872, Loss: 6.525181379402056e-05\n",
      "Batch: 876, Loss: 0.0005606373306363821\n",
      "Batch: 880, Loss: 0.00042913894867524505\n",
      "Batch: 884, Loss: 0.00013746385229751468\n",
      "Batch: 888, Loss: 0.00019229701138101518\n",
      "Batch: 892, Loss: 0.00045789685100317\n",
      "Batch: 896, Loss: 9.511566895525903e-05\n",
      "Batch: 900, Loss: 0.0004981820238754153\n",
      "Batch: 904, Loss: 0.00021433865185827017\n",
      "Batch: 908, Loss: 0.0012355651706457138\n",
      "Batch: 912, Loss: 5.837322532897815e-05\n",
      "Batch: 916, Loss: 0.0016436034347862005\n",
      "Batch: 920, Loss: 0.004195958375930786\n",
      "Batch: 924, Loss: 0.0026695765554904938\n",
      "Batch: 928, Loss: 0.006076198071241379\n",
      "Batch: 932, Loss: 0.00013443305215332657\n",
      "Batch: 936, Loss: 0.019964516162872314\n",
      "\n",
      "验证集: Average loss: 0.0446, Accuracy: 9902/10000 (99%)\n",
      "\n",
      "Model weights saved to logs/model_weights_epoch3.pth\n",
      "Epoch 5\n",
      "Batch: 0, Loss: 0.00304397358559072\n",
      "Batch: 4, Loss: 0.00022248970344662666\n",
      "Batch: 8, Loss: 0.00047358637675642967\n",
      "Batch: 12, Loss: 0.0005864594713784754\n",
      "Batch: 16, Loss: 0.007126166485249996\n",
      "Batch: 20, Loss: 4.06499020755291e-05\n",
      "Batch: 24, Loss: 0.0011372629087418318\n",
      "Batch: 28, Loss: 0.00039838533848524094\n",
      "Batch: 32, Loss: 0.0005703332717530429\n",
      "Batch: 36, Loss: 0.0015926634659990668\n",
      "Batch: 40, Loss: 0.0278148353099823\n",
      "Batch: 44, Loss: 0.020585937425494194\n",
      "Batch: 48, Loss: 0.00014141308201942593\n",
      "Batch: 52, Loss: 0.20391130447387695\n",
      "Batch: 56, Loss: 0.00011009651643689722\n",
      "Batch: 60, Loss: 0.005956410430371761\n",
      "Batch: 64, Loss: 8.177327254088596e-05\n",
      "Batch: 68, Loss: 0.00022026289661880583\n",
      "Batch: 72, Loss: 0.0001499214704381302\n",
      "Batch: 76, Loss: 0.020160676911473274\n",
      "Batch: 80, Loss: 0.007389797829091549\n",
      "Batch: 84, Loss: 3.1760017009219155e-05\n",
      "Batch: 88, Loss: 0.002533562248572707\n",
      "Batch: 92, Loss: 0.0005723976064473391\n",
      "Batch: 96, Loss: 0.04581647366285324\n",
      "Batch: 100, Loss: 2.855945422197692e-05\n",
      "Batch: 104, Loss: 0.0018558523152023554\n",
      "Batch: 108, Loss: 0.000767851248383522\n",
      "Batch: 112, Loss: 0.0024339857045561075\n",
      "Batch: 116, Loss: 0.00012497130956035107\n",
      "Batch: 120, Loss: 0.002749828854575753\n",
      "Batch: 124, Loss: 3.410071803955361e-05\n",
      "Batch: 128, Loss: 1.1183848982909694e-05\n",
      "Batch: 132, Loss: 0.00021374993957579136\n",
      "Batch: 136, Loss: 0.0006806760793551803\n",
      "Batch: 140, Loss: 1.732221971906256e-05\n",
      "Batch: 144, Loss: 0.01837499812245369\n",
      "Batch: 148, Loss: 0.021193744614720345\n",
      "Batch: 152, Loss: 0.00014416725025512278\n",
      "Batch: 156, Loss: 0.0002125946048181504\n",
      "Batch: 160, Loss: 0.00017131117056123912\n",
      "Batch: 164, Loss: 0.0001554677728563547\n",
      "Batch: 168, Loss: 0.030923567712306976\n",
      "Batch: 172, Loss: 0.0018302046228200197\n",
      "Batch: 176, Loss: 0.040861889719963074\n",
      "Batch: 180, Loss: 6.335930083878338e-05\n",
      "Batch: 184, Loss: 0.0002121480938512832\n",
      "Batch: 188, Loss: 0.012976112775504589\n",
      "Batch: 192, Loss: 0.00016656942898407578\n",
      "Batch: 196, Loss: 0.0004576561041176319\n",
      "Batch: 200, Loss: 2.526005482650362e-05\n",
      "Batch: 204, Loss: 0.006756789516657591\n",
      "Batch: 208, Loss: 0.0009796388912945986\n",
      "Batch: 212, Loss: 0.009178809821605682\n",
      "Batch: 216, Loss: 0.0001560358505230397\n",
      "Batch: 220, Loss: 2.1317933715181425e-05\n",
      "Batch: 224, Loss: 6.834409578004852e-05\n",
      "Batch: 228, Loss: 0.00014008249854668975\n",
      "Batch: 232, Loss: 0.017481250688433647\n",
      "Batch: 236, Loss: 0.0001571714092278853\n",
      "Batch: 240, Loss: 0.0014634868130087852\n",
      "Batch: 244, Loss: 0.0009175073937512934\n",
      "Batch: 248, Loss: 0.00027042595320381224\n",
      "Batch: 252, Loss: 0.003930260427296162\n",
      "Batch: 256, Loss: 5.3048635891173035e-05\n",
      "Batch: 260, Loss: 6.486847996711731e-05\n",
      "Batch: 264, Loss: 0.005916151218116283\n",
      "Batch: 268, Loss: 8.5748870333191e-06\n",
      "Batch: 272, Loss: 6.539207970490679e-05\n",
      "Batch: 276, Loss: 0.03621150553226471\n",
      "Batch: 280, Loss: 0.00015186438395176083\n",
      "Batch: 284, Loss: 0.0018193777650594711\n",
      "Batch: 288, Loss: 0.0001672029320616275\n",
      "Batch: 292, Loss: 8.442818398179952e-06\n",
      "Batch: 296, Loss: 7.772404205752537e-05\n",
      "Batch: 300, Loss: 7.914793241070583e-05\n",
      "Batch: 304, Loss: 0.0002920913393609226\n",
      "Batch: 308, Loss: 0.0001272116496693343\n",
      "Batch: 312, Loss: 0.0002860954264178872\n",
      "Batch: 316, Loss: 0.0017784221563488245\n",
      "Batch: 320, Loss: 1.2034880455757957e-05\n",
      "Batch: 324, Loss: 0.005857833661139011\n",
      "Batch: 328, Loss: 0.0015593296848237514\n",
      "Batch: 332, Loss: 5.091386992717162e-05\n",
      "Batch: 336, Loss: 0.004521939437836409\n",
      "Batch: 340, Loss: 1.8936460037366487e-05\n",
      "Batch: 344, Loss: 0.001997885527089238\n",
      "Batch: 348, Loss: 0.00045425741700455546\n",
      "Batch: 352, Loss: 0.0003097285807598382\n",
      "Batch: 356, Loss: 2.7959380531683564e-05\n",
      "Batch: 360, Loss: 0.00013302749721333385\n",
      "Batch: 364, Loss: 5.9786693782371e-06\n",
      "Batch: 368, Loss: 0.0005107187898829579\n",
      "Batch: 372, Loss: 3.544922947185114e-05\n",
      "Batch: 376, Loss: 0.02258770726621151\n",
      "Batch: 380, Loss: 0.00010252721403958276\n",
      "Batch: 384, Loss: 3.792824281845242e-05\n",
      "Batch: 388, Loss: 0.0007249457994475961\n",
      "Batch: 392, Loss: 2.127358857251238e-05\n",
      "Batch: 396, Loss: 0.0003108926466666162\n",
      "Batch: 400, Loss: 7.300779543584213e-05\n",
      "Batch: 404, Loss: 0.0006056898855604231\n",
      "Batch: 408, Loss: 8.055585203692317e-05\n",
      "Batch: 412, Loss: 0.02139393240213394\n",
      "Batch: 416, Loss: 0.000761758885346353\n",
      "Batch: 420, Loss: 0.0017183144809678197\n",
      "Batch: 424, Loss: 8.177765994332731e-05\n",
      "Batch: 428, Loss: 0.005320065189152956\n",
      "Batch: 432, Loss: 1.5919913494144566e-05\n",
      "Batch: 436, Loss: 0.00041992240585386753\n",
      "Batch: 440, Loss: 0.008683949708938599\n",
      "Batch: 444, Loss: 0.00027414897340349853\n",
      "Batch: 448, Loss: 0.0012666842667385936\n",
      "Batch: 452, Loss: 0.027698377147316933\n",
      "Batch: 456, Loss: 6.692502938676625e-05\n",
      "Batch: 460, Loss: 0.003189119277521968\n",
      "Batch: 464, Loss: 0.00026497969520278275\n",
      "Batch: 468, Loss: 2.955818854388781e-05\n",
      "Batch: 472, Loss: 0.0017227147473022342\n",
      "Batch: 476, Loss: 1.3169295016268734e-05\n",
      "Batch: 480, Loss: 0.00063126883469522\n",
      "Batch: 484, Loss: 2.1596031729131937e-05\n",
      "Batch: 488, Loss: 0.0004692957445513457\n",
      "Batch: 492, Loss: 0.0022794422693550587\n",
      "Batch: 496, Loss: 0.008455685339868069\n",
      "Batch: 500, Loss: 0.02137734554708004\n",
      "Batch: 504, Loss: 0.00035405505332164466\n",
      "Batch: 508, Loss: 0.0001526305131847039\n",
      "Batch: 512, Loss: 0.004841112531721592\n",
      "Batch: 516, Loss: 0.003010920947417617\n",
      "Batch: 520, Loss: 0.0001325353077845648\n",
      "Batch: 524, Loss: 4.02688556278008e-06\n",
      "Batch: 528, Loss: 0.09497072547674179\n",
      "Batch: 532, Loss: 0.00024306932755280286\n",
      "Batch: 536, Loss: 0.015886757522821426\n",
      "Batch: 540, Loss: 0.06413928419351578\n",
      "Batch: 544, Loss: 0.00030306316330097616\n",
      "Batch: 548, Loss: 3.9411952457157895e-05\n",
      "Batch: 552, Loss: 3.8663551094941795e-05\n",
      "Batch: 556, Loss: 0.00010854961874429137\n",
      "Batch: 560, Loss: 6.85393315507099e-05\n",
      "Batch: 564, Loss: 0.12195577472448349\n",
      "Batch: 568, Loss: 5.469009556691162e-05\n",
      "Batch: 572, Loss: 0.009512266144156456\n",
      "Batch: 576, Loss: 0.11662033200263977\n",
      "Batch: 580, Loss: 0.11113981157541275\n",
      "Batch: 584, Loss: 0.0006233945023268461\n",
      "Batch: 588, Loss: 0.0014117801329120994\n",
      "Batch: 592, Loss: 0.00011740595800802112\n",
      "Batch: 596, Loss: 0.00029873650055378675\n",
      "Batch: 600, Loss: 0.0006951778777875006\n",
      "Batch: 604, Loss: 3.1848394428379834e-05\n",
      "Batch: 608, Loss: 0.0038244081661105156\n",
      "Batch: 612, Loss: 0.006771473679691553\n",
      "Batch: 616, Loss: 0.032538339495658875\n",
      "Batch: 620, Loss: 5.409100413089618e-05\n",
      "Batch: 624, Loss: 0.0072755366563797\n",
      "Batch: 628, Loss: 0.007000763900578022\n",
      "Batch: 632, Loss: 0.005134460516273975\n",
      "Batch: 636, Loss: 0.00010328094504075125\n",
      "Batch: 640, Loss: 0.001882643555290997\n",
      "Batch: 644, Loss: 0.007913695648312569\n",
      "Batch: 648, Loss: 0.0015329169109463692\n",
      "Batch: 652, Loss: 0.0014407787239179015\n",
      "Batch: 656, Loss: 0.0004653059004340321\n",
      "Batch: 660, Loss: 0.008521384559571743\n",
      "Batch: 664, Loss: 0.0029602404683828354\n",
      "Batch: 668, Loss: 0.0030268123373389244\n",
      "Batch: 672, Loss: 0.004793792497366667\n",
      "Batch: 676, Loss: 6.515830318676308e-05\n",
      "Batch: 680, Loss: 0.00041129658347927034\n",
      "Batch: 684, Loss: 0.005841237958520651\n",
      "Batch: 688, Loss: 0.00763177452608943\n",
      "Batch: 692, Loss: 0.009600487537682056\n",
      "Batch: 696, Loss: 7.013753202045336e-05\n",
      "Batch: 700, Loss: 0.011606412939727306\n",
      "Batch: 704, Loss: 2.369844696659129e-05\n",
      "Batch: 708, Loss: 0.0031846370548009872\n",
      "Batch: 712, Loss: 7.916217145975679e-05\n",
      "Batch: 716, Loss: 2.3571838028146885e-05\n",
      "Batch: 720, Loss: 0.02830086275935173\n",
      "Batch: 724, Loss: 0.00022350707149598747\n",
      "Batch: 728, Loss: 0.003223316976800561\n",
      "Batch: 732, Loss: 0.0003795161028392613\n",
      "Batch: 736, Loss: 0.00027289503486827016\n",
      "Batch: 740, Loss: 3.3195461583090946e-05\n",
      "Batch: 744, Loss: 4.3086572986794636e-05\n",
      "Batch: 748, Loss: 0.0012291631428524852\n",
      "Batch: 752, Loss: 0.0025816867128014565\n",
      "Batch: 756, Loss: 0.0007632246124558151\n",
      "Batch: 760, Loss: 0.0002109038468915969\n",
      "Batch: 764, Loss: 0.0021971899550408125\n",
      "Batch: 768, Loss: 0.006220737937837839\n",
      "Batch: 772, Loss: 0.0029614055529236794\n",
      "Batch: 776, Loss: 0.02809656597673893\n",
      "Batch: 780, Loss: 0.0022434035781770945\n",
      "Batch: 784, Loss: 0.0001623480929993093\n",
      "Batch: 788, Loss: 8.320338383782655e-05\n",
      "Batch: 792, Loss: 0.014123908244073391\n",
      "Batch: 796, Loss: 0.00011972023639827967\n",
      "Batch: 800, Loss: 4.792177787749097e-05\n",
      "Batch: 804, Loss: 0.0043641915544867516\n",
      "Batch: 808, Loss: 6.464817033702275e-06\n",
      "Batch: 812, Loss: 4.6175773604772985e-05\n",
      "Batch: 816, Loss: 0.00271781487390399\n",
      "Batch: 820, Loss: 0.00022799335420131683\n",
      "Batch: 824, Loss: 6.957945151953027e-05\n",
      "Batch: 828, Loss: 2.3158710973802954e-05\n",
      "Batch: 832, Loss: 7.332026871154085e-05\n",
      "Batch: 836, Loss: 0.013387314043939114\n",
      "Batch: 840, Loss: 0.0003520009049680084\n",
      "Batch: 844, Loss: 9.675017645349726e-05\n",
      "Batch: 848, Loss: 4.1497980419080704e-05\n",
      "Batch: 852, Loss: 6.76051204209216e-05\n",
      "Batch: 856, Loss: 0.006202955264598131\n",
      "Batch: 860, Loss: 4.576321316562826e-06\n",
      "Batch: 864, Loss: 0.0008657746948301792\n",
      "Batch: 868, Loss: 0.00044486657134257257\n",
      "Batch: 872, Loss: 0.0043453434482216835\n",
      "Batch: 876, Loss: 0.00043090994586236775\n",
      "Batch: 880, Loss: 0.0017548109171912074\n",
      "Batch: 884, Loss: 0.00018285264377482235\n",
      "Batch: 888, Loss: 4.019743937533349e-05\n",
      "Batch: 892, Loss: 6.1929194998811e-06\n",
      "Batch: 896, Loss: 0.004744673613458872\n",
      "Batch: 900, Loss: 0.0022952884901314974\n",
      "Batch: 904, Loss: 7.744948379695415e-05\n",
      "Batch: 908, Loss: 5.317097020451911e-06\n",
      "Batch: 912, Loss: 0.002570306183770299\n",
      "Batch: 916, Loss: 0.01125830877572298\n",
      "Batch: 920, Loss: 5.435035200207494e-05\n",
      "Batch: 924, Loss: 2.2522835934069008e-05\n",
      "Batch: 928, Loss: 3.696960266097449e-05\n",
      "Batch: 932, Loss: 3.4788234188454226e-05\n",
      "Batch: 936, Loss: 0.0020118963439017534\n",
      "\n",
      "验证集: Average loss: 0.0556, Accuracy: 9879/10000 (99%)\n",
      "\n",
      "Model weights saved to logs/model_weights_epoch4.pth\n",
      "Epoch 6\n",
      "Batch: 0, Loss: 2.527960532461293e-05\n",
      "Batch: 4, Loss: 0.0006596232415176928\n",
      "Batch: 8, Loss: 1.3987702914164402e-05\n",
      "Batch: 12, Loss: 5.253056588117033e-05\n",
      "Batch: 16, Loss: 0.0024996064603328705\n",
      "Batch: 20, Loss: 2.592948658275418e-05\n",
      "Batch: 24, Loss: 4.77928278996842e-06\n",
      "Batch: 28, Loss: 2.7845978820550954e-06\n",
      "Batch: 32, Loss: 0.00027686136309057474\n",
      "Batch: 36, Loss: 4.411839836393483e-05\n",
      "Batch: 40, Loss: 0.00022991553123574704\n",
      "Batch: 44, Loss: 0.001318017952144146\n",
      "Batch: 48, Loss: 0.0001896735338959843\n",
      "Batch: 52, Loss: 0.000205143223865889\n",
      "Batch: 56, Loss: 0.00017369887791574\n",
      "Batch: 60, Loss: 0.001910841092467308\n",
      "Batch: 64, Loss: 0.0005384650430642068\n",
      "Batch: 68, Loss: 0.00023148139007389545\n",
      "Batch: 72, Loss: 0.0005092091159895062\n",
      "Batch: 76, Loss: 4.099528268852737e-06\n",
      "Batch: 80, Loss: 1.1398276001273189e-05\n",
      "Batch: 84, Loss: 2.468235106789507e-05\n",
      "Batch: 88, Loss: 0.001650652615353465\n",
      "Batch: 92, Loss: 1.3070455679553561e-05\n",
      "Batch: 96, Loss: 1.1592036571528297e-05\n",
      "Batch: 100, Loss: 0.0003117948945146054\n",
      "Batch: 104, Loss: 1.476124543842161e-05\n",
      "Batch: 108, Loss: 8.091818017419428e-06\n",
      "Batch: 112, Loss: 0.0003960092435590923\n",
      "Batch: 116, Loss: 7.2523239396105055e-06\n",
      "Batch: 120, Loss: 8.544544834876433e-05\n",
      "Batch: 124, Loss: 0.00032168003963306546\n",
      "Batch: 128, Loss: 4.393765266286209e-06\n",
      "Batch: 132, Loss: 4.118071956327185e-06\n",
      "Batch: 136, Loss: 0.000114224741992075\n",
      "Batch: 140, Loss: 6.1018599808448926e-05\n",
      "Batch: 144, Loss: 0.00015990363317541778\n",
      "Batch: 148, Loss: 6.409486377378926e-05\n",
      "Batch: 152, Loss: 0.001939864712767303\n",
      "Batch: 156, Loss: 0.023775288835167885\n",
      "Batch: 160, Loss: 3.8013895391486585e-05\n",
      "Batch: 164, Loss: 1.8764523701975122e-05\n",
      "Batch: 168, Loss: 0.0007536161574535072\n",
      "Batch: 172, Loss: 0.00022308476036414504\n",
      "Batch: 176, Loss: 8.002831600606441e-05\n",
      "Batch: 180, Loss: 0.0001988432341022417\n",
      "Batch: 184, Loss: 0.0005088597536087036\n",
      "Batch: 188, Loss: 4.214972250338178e-06\n",
      "Batch: 192, Loss: 0.0015506526688113809\n",
      "Batch: 196, Loss: 0.0067960117012262344\n",
      "Batch: 200, Loss: 0.000547097297385335\n",
      "Batch: 204, Loss: 9.026337465911638e-06\n",
      "Batch: 208, Loss: 0.003926683682948351\n",
      "Batch: 212, Loss: 0.00035297105205245316\n",
      "Batch: 216, Loss: 0.0002679081226233393\n",
      "Batch: 220, Loss: 0.004992156755179167\n",
      "Batch: 224, Loss: 0.001221218379214406\n",
      "Batch: 228, Loss: 0.00010761848534457386\n",
      "Batch: 232, Loss: 0.010492879897356033\n",
      "Batch: 236, Loss: 0.0003156373568344861\n",
      "Batch: 240, Loss: 7.022140380286146e-07\n",
      "Batch: 244, Loss: 0.15319059789180756\n",
      "Batch: 248, Loss: 0.0012589942198246717\n",
      "Batch: 252, Loss: 0.00014619338617194444\n",
      "Batch: 256, Loss: 0.00010606354044284672\n",
      "Batch: 260, Loss: 5.9354515542509034e-06\n",
      "Batch: 264, Loss: 0.005331969819962978\n",
      "Batch: 268, Loss: 0.07006349414587021\n",
      "Batch: 272, Loss: 1.8481769075151533e-05\n",
      "Batch: 276, Loss: 2.0103374481550418e-05\n",
      "Batch: 280, Loss: 2.6237599740852602e-05\n",
      "Batch: 284, Loss: 0.014898034743964672\n",
      "Batch: 288, Loss: 0.003636747831478715\n",
      "Batch: 292, Loss: 0.0001141195825766772\n",
      "Batch: 296, Loss: 0.00025625911075621843\n",
      "Batch: 300, Loss: 0.00018772596376948059\n",
      "Batch: 304, Loss: 0.00023467991559300572\n",
      "Batch: 308, Loss: 1.6162111933226697e-05\n",
      "Batch: 312, Loss: 0.05183510482311249\n",
      "Batch: 316, Loss: 0.00043476824066601694\n",
      "Batch: 320, Loss: 0.0062933932058513165\n",
      "Batch: 324, Loss: 1.0113393727806397e-05\n",
      "Batch: 328, Loss: 0.00010068838309962302\n",
      "Batch: 332, Loss: 8.283705028588884e-06\n",
      "Batch: 336, Loss: 5.2360377594595775e-05\n",
      "Batch: 340, Loss: 0.0006732286419719458\n",
      "Batch: 344, Loss: 3.024621219083201e-05\n",
      "Batch: 348, Loss: 0.00014391381409950554\n",
      "Batch: 352, Loss: 0.0008670658571645617\n",
      "Batch: 356, Loss: 0.004633473698049784\n",
      "Batch: 360, Loss: 0.007571510970592499\n",
      "Batch: 364, Loss: 0.0010469076223671436\n",
      "Batch: 368, Loss: 1.8037311747320928e-05\n",
      "Batch: 372, Loss: 4.557243755698437e-06\n",
      "Batch: 376, Loss: 0.00020731055701617151\n",
      "Batch: 380, Loss: 0.00011887502478202805\n",
      "Batch: 384, Loss: 0.00018460716819390655\n",
      "Batch: 388, Loss: 0.04394376650452614\n",
      "Batch: 392, Loss: 0.020327556878328323\n",
      "Batch: 396, Loss: 6.451910303439945e-05\n",
      "Batch: 400, Loss: 0.0007245487649925053\n",
      "Batch: 404, Loss: 2.4299610231537372e-05\n",
      "Batch: 408, Loss: 1.3244776710052975e-05\n",
      "Batch: 412, Loss: 1.5407431419589557e-05\n",
      "Batch: 416, Loss: 0.0008093396318145096\n",
      "Batch: 420, Loss: 2.5460132746957242e-05\n",
      "Batch: 424, Loss: 0.025465240702033043\n",
      "Batch: 428, Loss: 0.0004857832391280681\n",
      "Batch: 432, Loss: 0.0010022653732448816\n",
      "Batch: 436, Loss: 0.0008668455993756652\n",
      "Batch: 440, Loss: 2.462452175677754e-05\n",
      "Batch: 444, Loss: 0.0025118645280599594\n",
      "Batch: 448, Loss: 0.014721059240400791\n",
      "Batch: 452, Loss: 0.02571839466691017\n",
      "Batch: 456, Loss: 0.0003640585928224027\n",
      "Batch: 460, Loss: 0.00015876046381890774\n",
      "Batch: 464, Loss: 3.7767280446132645e-05\n",
      "Batch: 468, Loss: 0.10307787358760834\n",
      "Batch: 472, Loss: 0.0005874375347048044\n",
      "Batch: 476, Loss: 7.153422484407201e-05\n",
      "Batch: 480, Loss: 3.294460475444794e-05\n",
      "Batch: 484, Loss: 0.00010593383194645867\n",
      "Batch: 488, Loss: 4.83382391394116e-05\n",
      "Batch: 492, Loss: 0.0001569913438288495\n",
      "Batch: 496, Loss: 0.00034376519033685327\n",
      "Batch: 500, Loss: 0.0006985793588683009\n",
      "Batch: 504, Loss: 0.0016662790440022945\n",
      "Batch: 508, Loss: 2.640543971210718e-05\n",
      "Batch: 512, Loss: 0.006773571949452162\n",
      "Batch: 516, Loss: 2.3086739020072855e-05\n",
      "Batch: 520, Loss: 2.594892430352047e-05\n",
      "Batch: 524, Loss: 9.660305659053847e-05\n",
      "Batch: 528, Loss: 5.307668834575452e-05\n",
      "Batch: 532, Loss: 0.0001652265345910564\n",
      "Batch: 536, Loss: 0.0002919972757808864\n",
      "Batch: 540, Loss: 0.003114961087703705\n",
      "Batch: 544, Loss: 3.4879489248851314e-05\n",
      "Batch: 548, Loss: 0.0038076818455010653\n",
      "Batch: 552, Loss: 8.24727612780407e-06\n",
      "Batch: 556, Loss: 4.213133070152253e-05\n",
      "Batch: 560, Loss: 0.00011504696885822341\n",
      "Batch: 564, Loss: 0.013093709945678711\n",
      "Batch: 568, Loss: 0.0029203686863183975\n",
      "Batch: 572, Loss: 0.003416219027712941\n",
      "Batch: 576, Loss: 0.015491354279220104\n",
      "Batch: 580, Loss: 0.0001936673215823248\n",
      "Batch: 584, Loss: 0.030024245381355286\n",
      "Batch: 588, Loss: 0.0014993979129940271\n",
      "Batch: 592, Loss: 0.041348714381456375\n",
      "Batch: 596, Loss: 0.0009748917073011398\n",
      "Batch: 600, Loss: 0.0005646080244332552\n",
      "Batch: 604, Loss: 0.04219022020697594\n",
      "Batch: 608, Loss: 7.779797670082189e-06\n",
      "Batch: 612, Loss: 0.00010516892507439479\n",
      "Batch: 616, Loss: 0.0002292419521836564\n",
      "Batch: 620, Loss: 0.0005110302590765059\n",
      "Batch: 624, Loss: 7.40957329981029e-05\n",
      "Batch: 628, Loss: 0.005613194778561592\n",
      "Batch: 632, Loss: 3.5946904972661287e-06\n",
      "Batch: 636, Loss: 0.0001905417157104239\n",
      "Batch: 640, Loss: 0.003320017596706748\n",
      "Batch: 644, Loss: 0.00022855811403132975\n",
      "Batch: 648, Loss: 7.900683158368338e-06\n",
      "Batch: 652, Loss: 0.004618131555616856\n",
      "Batch: 656, Loss: 0.014926727861166\n",
      "Batch: 660, Loss: 0.00023576738021802157\n",
      "Batch: 664, Loss: 9.396247151016723e-06\n",
      "Batch: 668, Loss: 0.00447270879521966\n",
      "Batch: 672, Loss: 0.006452230270951986\n",
      "Batch: 676, Loss: 0.006316761020570993\n",
      "Batch: 680, Loss: 4.175897174718557e-06\n",
      "Batch: 684, Loss: 0.004606079775840044\n",
      "Batch: 688, Loss: 1.4508298590953927e-05\n",
      "Batch: 692, Loss: 0.0011914301430806518\n",
      "Batch: 696, Loss: 0.0042433724738657475\n",
      "Batch: 700, Loss: 6.593714942937368e-07\n",
      "Batch: 704, Loss: 1.0652479431882966e-05\n",
      "Batch: 708, Loss: 2.1810901671415195e-06\n",
      "Batch: 712, Loss: 0.00040496152359992266\n",
      "Batch: 716, Loss: 0.0030790469609200954\n",
      "Batch: 720, Loss: 0.06355390697717667\n",
      "Batch: 724, Loss: 0.0665634274482727\n",
      "Batch: 728, Loss: 0.00018683071539271623\n",
      "Batch: 732, Loss: 0.000625768443569541\n",
      "Batch: 736, Loss: 0.0015729693695902824\n",
      "Batch: 740, Loss: 0.0017145528690889478\n",
      "Batch: 744, Loss: 0.039088450372219086\n",
      "Batch: 748, Loss: 0.011092953383922577\n",
      "Batch: 752, Loss: 0.00031300471164286137\n",
      "Batch: 756, Loss: 7.023151556495577e-05\n",
      "Batch: 760, Loss: 4.40400508523453e-05\n",
      "Batch: 764, Loss: 5.906811566092074e-05\n",
      "Batch: 768, Loss: 0.017167797312140465\n",
      "Batch: 772, Loss: 3.1868657970335335e-05\n",
      "Batch: 776, Loss: 0.00023591575154569\n",
      "Batch: 780, Loss: 8.089958282653242e-05\n",
      "Batch: 784, Loss: 1.6614400010439567e-05\n",
      "Batch: 788, Loss: 0.008589652366936207\n",
      "Batch: 792, Loss: 0.020992042496800423\n",
      "Batch: 796, Loss: 0.041240207850933075\n",
      "Batch: 800, Loss: 0.0026749614626169205\n",
      "Batch: 804, Loss: 0.0007019786862656474\n",
      "Batch: 808, Loss: 5.8390221965964884e-05\n",
      "Batch: 812, Loss: 1.1069821994169615e-05\n",
      "Batch: 816, Loss: 0.018465226516127586\n",
      "Batch: 820, Loss: 0.00011750037083402276\n",
      "Batch: 824, Loss: 2.3803042495273985e-05\n",
      "Batch: 828, Loss: 0.002040931722149253\n",
      "Batch: 832, Loss: 7.732886842859443e-06\n",
      "Batch: 836, Loss: 0.0005227992660365999\n",
      "Batch: 840, Loss: 2.50156044785399e-05\n",
      "Batch: 844, Loss: 0.0007347246282733977\n",
      "Batch: 848, Loss: 0.0225234255194664\n",
      "Batch: 852, Loss: 0.0011911685578525066\n",
      "Batch: 856, Loss: 0.029096774756908417\n",
      "Batch: 860, Loss: 0.0001703784946585074\n",
      "Batch: 864, Loss: 0.003270935034379363\n",
      "Batch: 868, Loss: 6.503574695670977e-05\n",
      "Batch: 872, Loss: 1.622332320039277e-06\n",
      "Batch: 876, Loss: 0.03063698671758175\n",
      "Batch: 880, Loss: 1.9314778910484165e-05\n",
      "Batch: 884, Loss: 9.84440430329414e-06\n",
      "Batch: 888, Loss: 0.0013576780911535025\n",
      "Batch: 892, Loss: 9.077834693016484e-05\n",
      "Batch: 896, Loss: 1.760917075444013e-05\n",
      "Batch: 900, Loss: 0.00035964438575319946\n",
      "Batch: 904, Loss: 0.0001603709242772311\n",
      "Batch: 908, Loss: 0.00041572743793949485\n",
      "Batch: 912, Loss: 0.0053970166482031345\n",
      "Batch: 916, Loss: 0.00023683566541876644\n",
      "Batch: 920, Loss: 0.0001829314132919535\n",
      "Batch: 924, Loss: 1.216082091559656e-05\n",
      "Batch: 928, Loss: 1.2459564459277317e-05\n",
      "Batch: 932, Loss: 0.0039054779335856438\n",
      "Batch: 936, Loss: 0.0005607588100247085\n",
      "\n",
      "验证集: Average loss: 0.0431, Accuracy: 9915/10000 (99%)\n",
      "\n",
      "Model weights saved to logs/model_weights_epoch5.pth\n",
      "Epoch 7\n",
      "Batch: 0, Loss: 4.903911758447066e-05\n",
      "Batch: 4, Loss: 0.00012398502440191805\n",
      "Batch: 8, Loss: 3.882788587361574e-05\n",
      "Batch: 12, Loss: 6.543777999468148e-05\n",
      "Batch: 16, Loss: 0.040296074002981186\n",
      "Batch: 20, Loss: 0.013774450868368149\n",
      "Batch: 24, Loss: 4.805580715583346e-07\n",
      "Batch: 28, Loss: 0.00018129096133634448\n",
      "Batch: 32, Loss: 0.00031835242407396436\n",
      "Batch: 36, Loss: 0.00013639319513458759\n",
      "Batch: 40, Loss: 3.143462527077645e-05\n",
      "Batch: 44, Loss: 0.0017383435042575002\n",
      "Batch: 48, Loss: 0.01475419383496046\n",
      "Batch: 52, Loss: 0.038794104009866714\n",
      "Batch: 56, Loss: 0.0015303920954465866\n",
      "Batch: 60, Loss: 0.00020552592468447983\n",
      "Batch: 64, Loss: 4.959985744790174e-05\n",
      "Batch: 68, Loss: 7.034963346086442e-05\n",
      "Batch: 72, Loss: 9.512716133031063e-06\n",
      "Batch: 76, Loss: 0.019717929884791374\n",
      "Batch: 80, Loss: 0.00020962193957529962\n",
      "Batch: 84, Loss: 0.04938741400837898\n",
      "Batch: 88, Loss: 0.0034513084683567286\n",
      "Batch: 92, Loss: 6.79308723192662e-05\n",
      "Batch: 96, Loss: 0.0004672573122661561\n",
      "Batch: 100, Loss: 0.0001312521635554731\n",
      "Batch: 104, Loss: 0.0002188943763030693\n",
      "Batch: 108, Loss: 0.0008382753585465252\n",
      "Batch: 112, Loss: 0.000252796511631459\n",
      "Batch: 116, Loss: 0.008977742865681648\n",
      "Batch: 120, Loss: 5.9886398958042264e-05\n",
      "Batch: 124, Loss: 0.00042895914521068335\n",
      "Batch: 128, Loss: 0.0026742948684841394\n",
      "Batch: 132, Loss: 9.071947715710849e-06\n",
      "Batch: 136, Loss: 0.00022202888794708997\n",
      "Batch: 140, Loss: 0.0006959937745705247\n",
      "Batch: 144, Loss: 0.005507871974259615\n",
      "Batch: 148, Loss: 0.0005793757736682892\n",
      "Batch: 152, Loss: 0.10413950681686401\n",
      "Batch: 156, Loss: 0.007717471104115248\n",
      "Batch: 160, Loss: 0.0006098243175074458\n",
      "Batch: 164, Loss: 6.489000952569768e-06\n",
      "Batch: 168, Loss: 0.020507484674453735\n",
      "Batch: 172, Loss: 0.0010816819267347455\n",
      "Batch: 176, Loss: 0.023792341351509094\n",
      "Batch: 180, Loss: 0.000873134471476078\n",
      "Batch: 184, Loss: 0.0022482892964035273\n",
      "Batch: 188, Loss: 2.8780043066944927e-05\n",
      "Batch: 192, Loss: 3.390622077859007e-05\n",
      "Batch: 196, Loss: 0.00010458477481734008\n",
      "Batch: 200, Loss: 4.956102202413604e-06\n",
      "Batch: 204, Loss: 6.025335096637718e-06\n",
      "Batch: 208, Loss: 0.00016151368618011475\n",
      "Batch: 212, Loss: 0.001045626006089151\n",
      "Batch: 216, Loss: 0.0013578612124547362\n",
      "Batch: 220, Loss: 1.152000913862139e-05\n",
      "Batch: 224, Loss: 0.002143087098374963\n",
      "Batch: 228, Loss: 3.619940980570391e-05\n",
      "Batch: 232, Loss: 2.0728066374431364e-05\n",
      "Batch: 236, Loss: 3.02038843074115e-05\n",
      "Batch: 240, Loss: 4.7766156058060005e-05\n",
      "Batch: 244, Loss: 1.0921121429419145e-05\n",
      "Batch: 248, Loss: 0.01843179389834404\n",
      "Batch: 252, Loss: 0.0010437812888994813\n",
      "Batch: 256, Loss: 0.0006156840827316046\n",
      "Batch: 260, Loss: 0.00019890822295565158\n",
      "Batch: 264, Loss: 3.0396827241929714e-06\n",
      "Batch: 268, Loss: 0.005279480945318937\n",
      "Batch: 272, Loss: 6.51111213301192e-06\n",
      "Batch: 276, Loss: 2.5216595531674102e-05\n",
      "Batch: 280, Loss: 0.0032588427420705557\n",
      "Batch: 284, Loss: 0.014382107183337212\n",
      "Batch: 288, Loss: 6.417906206479529e-06\n",
      "Batch: 292, Loss: 0.0023359274491667747\n",
      "Batch: 296, Loss: 2.8087499686080264e-06\n",
      "Batch: 300, Loss: 0.0009433787781745195\n",
      "Batch: 304, Loss: 5.081007657281589e-06\n",
      "Batch: 308, Loss: 5.5150572734419256e-05\n",
      "Batch: 312, Loss: 0.0058891382068395615\n",
      "Batch: 316, Loss: 5.599120049737394e-05\n",
      "Batch: 320, Loss: 0.00033308175625279546\n",
      "Batch: 324, Loss: 1.2924005204695277e-05\n",
      "Batch: 328, Loss: 0.0014300207840278745\n",
      "Batch: 332, Loss: 0.006296111736446619\n",
      "Batch: 336, Loss: 4.981532401870936e-05\n",
      "Batch: 340, Loss: 0.0008221645839512348\n",
      "Batch: 344, Loss: 8.163136226357892e-05\n",
      "Batch: 348, Loss: 0.003281952580437064\n",
      "Batch: 352, Loss: 0.0013109503779560328\n",
      "Batch: 356, Loss: 1.6334841347998008e-05\n",
      "Batch: 360, Loss: 9.480699532105064e-07\n",
      "Batch: 364, Loss: 0.002803729847073555\n",
      "Batch: 368, Loss: 1.2721503708235105e-06\n",
      "Batch: 372, Loss: 0.00036257298779673874\n",
      "Batch: 376, Loss: 0.00931630190461874\n",
      "Batch: 380, Loss: 5.342300210031681e-05\n",
      "Batch: 384, Loss: 0.0058939955197274685\n",
      "Batch: 388, Loss: 1.0038431355496868e-05\n",
      "Batch: 392, Loss: 0.044445790350437164\n",
      "Batch: 396, Loss: 0.00023506965953856707\n",
      "Batch: 400, Loss: 0.005464490503072739\n",
      "Batch: 404, Loss: 2.285412620040006e-06\n",
      "Batch: 408, Loss: 0.0004433417343534529\n",
      "Batch: 412, Loss: 0.00014008286234457046\n",
      "Batch: 416, Loss: 0.027173487469553947\n",
      "Batch: 420, Loss: 5.4432483011623845e-05\n",
      "Batch: 424, Loss: 0.09765666723251343\n",
      "Batch: 428, Loss: 0.0011622002348303795\n",
      "Batch: 432, Loss: 0.03051595948636532\n",
      "Batch: 436, Loss: 0.010927343741059303\n",
      "Batch: 440, Loss: 0.00012833127402700484\n",
      "Batch: 444, Loss: 0.00016655905346851796\n",
      "Batch: 448, Loss: 4.880099595538923e-07\n",
      "Batch: 452, Loss: 4.3806708163174335e-06\n",
      "Batch: 456, Loss: 1.847424209699966e-05\n",
      "Batch: 460, Loss: 0.008757855743169785\n",
      "Batch: 464, Loss: 0.00040302445995621383\n",
      "Batch: 468, Loss: 1.2536048416222911e-05\n",
      "Batch: 472, Loss: 0.00045000031241215765\n",
      "Batch: 476, Loss: 0.002714472124353051\n",
      "Batch: 480, Loss: 6.04186461714562e-05\n",
      "Batch: 484, Loss: 0.0016795194242149591\n",
      "Batch: 488, Loss: 1.9706083094206406e-06\n",
      "Batch: 492, Loss: 5.765996320405975e-05\n",
      "Batch: 496, Loss: 0.0001270960201509297\n",
      "Batch: 500, Loss: 3.4494414649088867e-06\n",
      "Batch: 504, Loss: 0.012486308813095093\n",
      "Batch: 508, Loss: 0.0001441700878785923\n",
      "Batch: 512, Loss: 0.002536692190915346\n",
      "Batch: 516, Loss: 1.7879017832456157e-05\n",
      "Batch: 520, Loss: 2.309901719854679e-05\n",
      "Batch: 524, Loss: 0.003004518337547779\n",
      "Batch: 528, Loss: 5.773250450147316e-06\n",
      "Batch: 532, Loss: 3.1421674066223204e-05\n",
      "Batch: 536, Loss: 0.04267115518450737\n",
      "Batch: 540, Loss: 3.032034874195233e-05\n",
      "Batch: 544, Loss: 4.06627259508241e-05\n",
      "Batch: 548, Loss: 0.001283781137317419\n",
      "Batch: 552, Loss: 0.0008341890061274171\n",
      "Batch: 556, Loss: 3.227719207643531e-05\n",
      "Batch: 560, Loss: 0.00013014017895329744\n",
      "Batch: 564, Loss: 8.034834536374547e-06\n",
      "Batch: 568, Loss: 0.013278573751449585\n",
      "Batch: 572, Loss: 0.051822710782289505\n",
      "Batch: 576, Loss: 0.11013326793909073\n",
      "Batch: 580, Loss: 0.0002688082167878747\n",
      "Batch: 584, Loss: 0.0004272587539162487\n",
      "Batch: 588, Loss: 0.00042034214129671454\n",
      "Batch: 592, Loss: 0.001749012852087617\n",
      "Batch: 596, Loss: 0.0007926620310172439\n",
      "Batch: 600, Loss: 8.44125752337277e-05\n",
      "Batch: 604, Loss: 5.074141881777905e-05\n",
      "Batch: 608, Loss: 0.0009833768708631396\n",
      "Batch: 612, Loss: 1.5667628758819774e-05\n",
      "Batch: 616, Loss: 0.026988858357071877\n",
      "Batch: 620, Loss: 0.00022362879826687276\n",
      "Batch: 624, Loss: 1.6763749499659752e-07\n",
      "Batch: 628, Loss: 1.9314877590659307e-06\n",
      "Batch: 632, Loss: 0.014229324646294117\n",
      "Batch: 636, Loss: 0.0001862041826825589\n",
      "Batch: 640, Loss: 0.0004195634392090142\n",
      "Batch: 644, Loss: 4.2654440335354593e-07\n",
      "Batch: 648, Loss: 4.4616965169552714e-05\n",
      "Batch: 652, Loss: 2.786271579680033e-06\n",
      "Batch: 656, Loss: 2.238784873043187e-06\n",
      "Batch: 660, Loss: 0.0002409528533462435\n",
      "Batch: 664, Loss: 0.0006942455074749887\n",
      "Batch: 668, Loss: 8.854141924530268e-05\n",
      "Batch: 672, Loss: 0.00022233917843550444\n",
      "Batch: 676, Loss: 1.3224763506514137e-07\n",
      "Batch: 680, Loss: 6.533402483910322e-05\n",
      "Batch: 684, Loss: 1.141767484114098e-06\n",
      "Batch: 688, Loss: 0.003237670985981822\n",
      "Batch: 692, Loss: 0.00045535003300756216\n",
      "Batch: 696, Loss: 1.4342344911710825e-07\n",
      "Batch: 700, Loss: 0.0016542861703783274\n",
      "Batch: 704, Loss: 0.1255987584590912\n",
      "Batch: 708, Loss: 1.1101176369265886e-06\n",
      "Batch: 712, Loss: 0.00014687349903397262\n",
      "Batch: 716, Loss: 0.021558819338679314\n",
      "Batch: 720, Loss: 0.0515294186770916\n",
      "Batch: 724, Loss: 4.242665454512462e-06\n",
      "Batch: 728, Loss: 2.469727860443527e-06\n",
      "Batch: 732, Loss: 0.00013610615860670805\n",
      "Batch: 736, Loss: 0.00010240911797154695\n",
      "Batch: 740, Loss: 0.001019666320644319\n",
      "Batch: 744, Loss: 3.4084318940585945e-06\n",
      "Batch: 748, Loss: 1.4917485714249779e-05\n",
      "Batch: 752, Loss: 0.0012860584538429976\n",
      "Batch: 756, Loss: 0.001274020760320127\n",
      "Batch: 760, Loss: 1.4420411389437504e-05\n",
      "Batch: 764, Loss: 1.1219433872611262e-05\n",
      "Batch: 768, Loss: 0.00013325315376278013\n",
      "Batch: 772, Loss: 0.0002716306480579078\n",
      "Batch: 776, Loss: 0.003186261048540473\n",
      "Batch: 780, Loss: 0.0026469773147255182\n",
      "Batch: 784, Loss: 0.05108761042356491\n",
      "Batch: 788, Loss: 0.00039715791353955865\n",
      "Batch: 792, Loss: 0.01247237529605627\n",
      "Batch: 796, Loss: 0.019978400319814682\n",
      "Batch: 800, Loss: 0.005452730227261782\n",
      "Batch: 804, Loss: 0.000302414147881791\n",
      "Batch: 808, Loss: 0.00015987725055310875\n",
      "Batch: 812, Loss: 0.0004941861843690276\n",
      "Batch: 816, Loss: 0.00031698940438218415\n",
      "Batch: 820, Loss: 0.0019323620945215225\n",
      "Batch: 824, Loss: 0.00021443804143927991\n",
      "Batch: 828, Loss: 0.0018751061288639903\n",
      "Batch: 832, Loss: 0.0007907425751909614\n",
      "Batch: 836, Loss: 0.0002651350514497608\n",
      "Batch: 840, Loss: 6.856261461507529e-05\n",
      "Batch: 844, Loss: 0.0003846017934847623\n",
      "Batch: 848, Loss: 0.00010288303747074679\n",
      "Batch: 852, Loss: 4.041909562602086e-07\n",
      "Batch: 856, Loss: 0.007638375740498304\n",
      "Batch: 860, Loss: 2.978200200232095e-06\n",
      "Batch: 864, Loss: 6.922409374965355e-05\n",
      "Batch: 868, Loss: 4.149953383603133e-05\n",
      "Batch: 872, Loss: 6.894562375237001e-06\n",
      "Batch: 876, Loss: 0.00016997658531181514\n",
      "Batch: 880, Loss: 0.0007543640676885843\n",
      "Batch: 884, Loss: 0.0003614150045905262\n",
      "Batch: 888, Loss: 0.0008345650276169181\n",
      "Batch: 892, Loss: 0.00014502039994113147\n",
      "Batch: 896, Loss: 0.0009213319863192737\n",
      "Batch: 900, Loss: 0.0011801847722381353\n",
      "Batch: 904, Loss: 0.00017971487250179052\n",
      "Batch: 908, Loss: 0.0012892408994957805\n",
      "Batch: 912, Loss: 8.072103810263798e-06\n",
      "Batch: 916, Loss: 0.019944434985518456\n",
      "Batch: 920, Loss: 1.6521372572242399e-06\n",
      "Batch: 924, Loss: 0.0013464848743751645\n",
      "Batch: 928, Loss: 0.0004551555321086198\n",
      "Batch: 932, Loss: 2.3095469714462524e-06\n",
      "Batch: 936, Loss: 5.215385385781701e-07\n",
      "\n",
      "验证集: Average loss: 0.0498, Accuracy: 9904/10000 (99%)\n",
      "\n",
      "Model weights saved to logs/model_weights_epoch6.pth\n",
      "Epoch 8\n",
      "Batch: 0, Loss: 2.4753946945565986e-06\n",
      "Batch: 4, Loss: 0.0007714785169810057\n",
      "Batch: 8, Loss: 0.00014914132771082222\n",
      "Batch: 12, Loss: 1.940775518960436e-06\n",
      "Batch: 16, Loss: 0.00027786369901150465\n",
      "Batch: 20, Loss: 3.352623480168404e-06\n",
      "Batch: 24, Loss: 8.67732524056919e-06\n",
      "Batch: 28, Loss: 0.005035893060266972\n",
      "Batch: 32, Loss: 0.00011238643492106348\n",
      "Batch: 36, Loss: 0.00010623590787872672\n",
      "Batch: 40, Loss: 0.0006869742064736784\n",
      "Batch: 44, Loss: 7.140761681512231e-06\n",
      "Batch: 48, Loss: 4.676770913647488e-06\n",
      "Batch: 52, Loss: 3.635676694102585e-05\n",
      "Batch: 56, Loss: 3.803290837822715e-06\n",
      "Batch: 60, Loss: 0.04348874092102051\n",
      "Batch: 64, Loss: 0.00038581292028538883\n",
      "Batch: 68, Loss: 1.5283691027434543e-05\n",
      "Batch: 72, Loss: 0.012391181662678719\n",
      "Batch: 76, Loss: 3.986134834121913e-05\n",
      "Batch: 80, Loss: 0.007070028688758612\n",
      "Batch: 84, Loss: 6.042915629222989e-05\n",
      "Batch: 88, Loss: 0.0004630657786037773\n",
      "Batch: 92, Loss: 6.351886258926243e-05\n",
      "Batch: 96, Loss: 8.201233140425757e-05\n",
      "Batch: 100, Loss: 2.0494650016189553e-05\n",
      "Batch: 104, Loss: 0.006854817736893892\n",
      "Batch: 108, Loss: 0.000792363251093775\n",
      "Batch: 112, Loss: 0.02734803780913353\n",
      "Batch: 116, Loss: 0.0005225214408710599\n",
      "Batch: 120, Loss: 0.00012859136040788144\n",
      "Batch: 124, Loss: 6.591337296413258e-06\n",
      "Batch: 128, Loss: 4.514450847636908e-05\n",
      "Batch: 132, Loss: 2.9805232770740986e-05\n",
      "Batch: 136, Loss: 1.2938994586875197e-05\n",
      "Batch: 140, Loss: 4.3182404624531046e-05\n",
      "Batch: 144, Loss: 0.03232596442103386\n",
      "Batch: 148, Loss: 0.00013929045235272497\n",
      "Batch: 152, Loss: 3.8125426726765e-06\n",
      "Batch: 156, Loss: 2.657837285369169e-06\n",
      "Batch: 160, Loss: 3.170137915731175e-06\n",
      "Batch: 164, Loss: 5.0833790737669915e-05\n",
      "Batch: 168, Loss: 0.00016750940994825214\n",
      "Batch: 172, Loss: 0.00031921081244945526\n",
      "Batch: 176, Loss: 0.00013401382602751255\n",
      "Batch: 180, Loss: 3.5704972560779424e-06\n",
      "Batch: 184, Loss: 0.0007749351789243519\n",
      "Batch: 188, Loss: 6.500555969068955e-07\n",
      "Batch: 192, Loss: 0.00020797335309907794\n",
      "Batch: 196, Loss: 0.00020693150872830302\n",
      "Batch: 200, Loss: 0.00024394113279413432\n",
      "Batch: 204, Loss: 2.1420380846848275e-07\n",
      "Batch: 208, Loss: 7.367470971075818e-06\n",
      "Batch: 212, Loss: 0.011188491247594357\n",
      "Batch: 216, Loss: 0.0003237469936721027\n",
      "Batch: 220, Loss: 4.97675864608027e-06\n",
      "Batch: 224, Loss: 2.020241845457349e-05\n",
      "Batch: 228, Loss: 0.0019230800680816174\n",
      "Batch: 232, Loss: 0.008502553217113018\n",
      "Batch: 236, Loss: 8.73112276167376e-06\n",
      "Batch: 240, Loss: 1.061756756826071e-05\n",
      "Batch: 244, Loss: 6.0330541600706056e-05\n",
      "Batch: 248, Loss: 0.0016577558126300573\n",
      "Batch: 252, Loss: 3.892644599545747e-06\n",
      "Batch: 256, Loss: 1.1660036989269429e-06\n",
      "Batch: 260, Loss: 2.8608312732103514e-06\n",
      "Batch: 264, Loss: 3.2974916393868625e-05\n",
      "Batch: 268, Loss: 9.485172995482571e-06\n",
      "Batch: 272, Loss: 0.00033266987884417176\n",
      "Batch: 276, Loss: 0.0002959984412882477\n",
      "Batch: 280, Loss: 0.0008018945227377117\n",
      "Batch: 284, Loss: 1.2266983503650408e-05\n",
      "Batch: 288, Loss: 1.5844176232349128e-05\n",
      "Batch: 292, Loss: 9.577839227858931e-05\n",
      "Batch: 296, Loss: 0.08689334988594055\n",
      "Batch: 300, Loss: 0.0016705545131117105\n",
      "Batch: 304, Loss: 0.033208489418029785\n",
      "Batch: 308, Loss: 0.00013258492981549352\n",
      "Batch: 312, Loss: 7.7871391113149e-06\n",
      "Batch: 316, Loss: 0.00015046485350467265\n",
      "Batch: 320, Loss: 0.007884236052632332\n",
      "Batch: 324, Loss: 0.00019205034186597914\n",
      "Batch: 328, Loss: 1.682894435361959e-05\n",
      "Batch: 332, Loss: 7.130225276341662e-05\n",
      "Batch: 336, Loss: 0.000395124196074903\n",
      "Batch: 340, Loss: 8.863389666657895e-06\n",
      "Batch: 344, Loss: 0.0023293285630643368\n",
      "Batch: 348, Loss: 7.005820953054354e-05\n",
      "Batch: 352, Loss: 0.0004982653772458434\n",
      "Batch: 356, Loss: 6.217783084139228e-05\n",
      "Batch: 360, Loss: 2.130755319740274e-06\n",
      "Batch: 364, Loss: 3.756478690775111e-05\n",
      "Batch: 368, Loss: 0.0007657745154574513\n",
      "Batch: 372, Loss: 2.0864448742941022e-05\n",
      "Batch: 376, Loss: 5.87792328587966e-06\n",
      "Batch: 380, Loss: 0.001938488450832665\n",
      "Batch: 384, Loss: 0.018703922629356384\n",
      "Batch: 388, Loss: 8.114255251712166e-06\n",
      "Batch: 392, Loss: 6.779957857361296e-07\n",
      "Batch: 396, Loss: 0.003064829623326659\n",
      "Batch: 400, Loss: 0.0027830093167722225\n",
      "Batch: 404, Loss: 0.0036832261830568314\n",
      "Batch: 408, Loss: 3.078234658460133e-05\n",
      "Batch: 412, Loss: 0.0017398108029738069\n",
      "Batch: 416, Loss: 2.048903411377978e-07\n",
      "Batch: 420, Loss: 3.855352133541601e-06\n",
      "Batch: 424, Loss: 5.1150043873349205e-05\n",
      "Batch: 428, Loss: 1.797950972104445e-05\n",
      "Batch: 432, Loss: 0.0002369832363910973\n",
      "Batch: 436, Loss: 0.000252644153079018\n",
      "Batch: 440, Loss: 0.0002792236628010869\n",
      "Batch: 444, Loss: 0.0007035643211565912\n",
      "Batch: 448, Loss: 0.0007578360382467508\n",
      "Batch: 452, Loss: 0.0007807365036569536\n",
      "Batch: 456, Loss: 0.009059309028089046\n",
      "Batch: 460, Loss: 1.773177814357041e-06\n",
      "Batch: 464, Loss: 1.2367715953587322e-06\n",
      "Batch: 468, Loss: 0.03536786139011383\n",
      "Batch: 472, Loss: 4.125193299842067e-05\n",
      "Batch: 476, Loss: 0.0005242082406766713\n",
      "Batch: 480, Loss: 0.00010412736446596682\n",
      "Batch: 484, Loss: 0.005890401545912027\n",
      "Batch: 488, Loss: 0.0004382808692753315\n",
      "Batch: 492, Loss: 5.766473805124406e-06\n",
      "Batch: 496, Loss: 4.0193590393755585e-06\n",
      "Batch: 500, Loss: 1.6412532204412855e-05\n",
      "Batch: 504, Loss: 0.0006319681415334344\n",
      "Batch: 508, Loss: 1.7098601574616623e-06\n",
      "Batch: 512, Loss: 3.019373798451852e-05\n",
      "Batch: 516, Loss: 5.112378130434081e-05\n",
      "Batch: 520, Loss: 0.0017521888948976994\n",
      "Batch: 524, Loss: 0.015244155190885067\n",
      "Batch: 528, Loss: 2.042587220785208e-05\n",
      "Batch: 532, Loss: 0.023052886128425598\n",
      "Batch: 536, Loss: 3.291158009233186e-06\n",
      "Batch: 540, Loss: 0.019776316359639168\n",
      "Batch: 544, Loss: 0.000299894338240847\n",
      "Batch: 548, Loss: 0.00019758685084525496\n",
      "Batch: 552, Loss: 0.00013240518455859274\n",
      "Batch: 556, Loss: 8.494796202285215e-05\n",
      "Batch: 560, Loss: 0.0006832422805018723\n",
      "Batch: 564, Loss: 0.0019855995196849108\n",
      "Batch: 568, Loss: 3.527675744408043e-06\n",
      "Batch: 572, Loss: 0.0005407530115917325\n",
      "Batch: 576, Loss: 0.0012885977048426867\n",
      "Batch: 580, Loss: 7.529548838647315e-06\n",
      "Batch: 584, Loss: 3.565894076018594e-05\n",
      "Batch: 588, Loss: 0.002605715999379754\n",
      "Batch: 592, Loss: 3.3190601698152022e-06\n",
      "Batch: 596, Loss: 1.3924992344982456e-05\n",
      "Batch: 600, Loss: 3.602818105719052e-05\n",
      "Batch: 604, Loss: 9.041339581017382e-06\n",
      "Batch: 608, Loss: 5.848879663972184e-05\n",
      "Batch: 612, Loss: 0.0075448681600391865\n",
      "Batch: 616, Loss: 0.00030358840012922883\n",
      "Batch: 620, Loss: 0.0010397817241027951\n",
      "Batch: 624, Loss: 0.0006638953345827758\n",
      "Batch: 628, Loss: 1.7266461327380966e-06\n",
      "Batch: 632, Loss: 0.04389185085892677\n",
      "Batch: 636, Loss: 0.008916964754462242\n",
      "Batch: 640, Loss: 1.072875988938904e-06\n",
      "Batch: 644, Loss: 0.0013193044578656554\n",
      "Batch: 648, Loss: 2.8520529667730443e-05\n",
      "Batch: 652, Loss: 4.362261825008318e-05\n",
      "Batch: 656, Loss: 0.020276613533496857\n",
      "Batch: 660, Loss: 3.07280060951598e-05\n",
      "Batch: 664, Loss: 4.605521826306358e-05\n",
      "Batch: 668, Loss: 0.0007097266498021781\n",
      "Batch: 672, Loss: 2.451650652801618e-05\n",
      "Batch: 676, Loss: 5.863040314579848e-06\n",
      "Batch: 680, Loss: 3.608559200074524e-05\n",
      "Batch: 684, Loss: 6.1019665736239403e-05\n",
      "Batch: 688, Loss: 0.0011399748036637902\n",
      "Batch: 692, Loss: 6.780738203815417e-06\n",
      "Batch: 696, Loss: 2.8991895305807702e-05\n",
      "Batch: 700, Loss: 2.3846976546337828e-05\n",
      "Batch: 704, Loss: 0.0013223076239228249\n",
      "Batch: 708, Loss: 6.952483090572059e-05\n",
      "Batch: 712, Loss: 0.0015805801376700401\n",
      "Batch: 716, Loss: 4.284068495508109e-07\n",
      "Batch: 720, Loss: 0.0002811848826240748\n",
      "Batch: 724, Loss: 3.0565211091015954e-06\n",
      "Batch: 728, Loss: 0.0014881864190101624\n",
      "Batch: 732, Loss: 0.045048411935567856\n",
      "Batch: 736, Loss: 0.09079954773187637\n",
      "Batch: 740, Loss: 0.0006652750307694077\n",
      "Batch: 744, Loss: 0.00043004288454540074\n",
      "Batch: 748, Loss: 0.025606095790863037\n",
      "Batch: 752, Loss: 0.00012689504364971071\n",
      "Batch: 756, Loss: 0.0009135667933151126\n",
      "Batch: 760, Loss: 4.0064947825158015e-05\n",
      "Batch: 764, Loss: 4.5736269385088235e-05\n",
      "Batch: 768, Loss: 0.00015648352564312518\n",
      "Batch: 772, Loss: 2.607634996820707e-06\n",
      "Batch: 776, Loss: 0.0001379052409902215\n",
      "Batch: 780, Loss: 4.598875602823682e-05\n",
      "Batch: 784, Loss: 0.0006344103603623807\n",
      "Batch: 788, Loss: 0.00016355024126823992\n",
      "Batch: 792, Loss: 3.4904231142718345e-05\n",
      "Batch: 796, Loss: 0.004692177753895521\n",
      "Batch: 800, Loss: 0.0002646541688591242\n",
      "Batch: 804, Loss: 1.0523688615649007e-05\n",
      "Batch: 808, Loss: 1.3857585372534231e-06\n",
      "Batch: 812, Loss: 0.00022300879936665297\n",
      "Batch: 816, Loss: 0.00011739364708773792\n",
      "Batch: 820, Loss: 8.678785525262356e-05\n",
      "Batch: 824, Loss: 4.513487874646671e-05\n",
      "Batch: 828, Loss: 2.7574278647080064e-05\n",
      "Batch: 832, Loss: 0.00010188233136432245\n",
      "Batch: 836, Loss: 0.02308172918856144\n",
      "Batch: 840, Loss: 1.0994144759024493e-05\n",
      "Batch: 844, Loss: 0.007222854532301426\n",
      "Batch: 848, Loss: 0.00019903609063476324\n",
      "Batch: 852, Loss: 0.0005663793417625129\n",
      "Batch: 856, Loss: 1.7135708958448959e-06\n",
      "Batch: 860, Loss: 0.00012357468949630857\n",
      "Batch: 864, Loss: 0.01317943911999464\n",
      "Batch: 868, Loss: 1.7936422409547959e-06\n",
      "Batch: 872, Loss: 0.0003911799576599151\n",
      "Batch: 876, Loss: 7.150002147682244e-06\n",
      "Batch: 880, Loss: 8.245430944953114e-05\n",
      "Batch: 884, Loss: 1.704840178717859e-05\n",
      "Batch: 888, Loss: 3.4445027267793193e-05\n",
      "Batch: 892, Loss: 4.364566848380491e-05\n",
      "Batch: 896, Loss: 1.0491088687558658e-05\n",
      "Batch: 900, Loss: 0.0009818377438932657\n",
      "Batch: 904, Loss: 0.00018778102821670473\n",
      "Batch: 908, Loss: 0.0006727935397066176\n",
      "Batch: 912, Loss: 4.388083652884234e-06\n",
      "Batch: 916, Loss: 0.0004560838860925287\n",
      "Batch: 920, Loss: 4.7561497922288254e-05\n",
      "Batch: 924, Loss: 0.002529514953494072\n",
      "Batch: 928, Loss: 0.00029177835676819086\n",
      "Batch: 932, Loss: 0.0028838408179581165\n",
      "Batch: 936, Loss: 3.620799361669924e-06\n",
      "\n",
      "验证集: Average loss: 0.0567, Accuracy: 9884/10000 (99%)\n",
      "\n",
      "Model weights saved to logs/model_weights_epoch7.pth\n",
      "Epoch 9\n",
      "Batch: 0, Loss: 0.01015065610408783\n",
      "Batch: 4, Loss: 0.00017273379489779472\n",
      "Batch: 8, Loss: 4.7369256208185107e-05\n",
      "Batch: 12, Loss: 0.00029146583983674645\n",
      "Batch: 16, Loss: 3.3154046832351014e-06\n",
      "Batch: 20, Loss: 3.5576368873080355e-07\n",
      "Batch: 24, Loss: 0.002891258103772998\n",
      "Batch: 28, Loss: 0.0005835383199155331\n",
      "Batch: 32, Loss: 4.201568117423449e-06\n",
      "Batch: 36, Loss: 0.0009675294277258217\n",
      "Batch: 40, Loss: 2.888751168939052e-06\n",
      "Batch: 44, Loss: 5.0005444791167974e-05\n",
      "Batch: 48, Loss: 0.001163111417554319\n",
      "Batch: 52, Loss: 6.174115696921945e-06\n",
      "Batch: 56, Loss: 0.0013016324955970049\n",
      "Batch: 60, Loss: 5.182309905649163e-05\n",
      "Batch: 64, Loss: 1.136191826844879e-06\n",
      "Batch: 68, Loss: 0.00011150601494591683\n",
      "Batch: 72, Loss: 0.002573735313490033\n",
      "Batch: 76, Loss: 1.6552598026464693e-05\n",
      "Batch: 80, Loss: 5.237327968643513e-06\n",
      "Batch: 84, Loss: 2.091700253004092e-06\n",
      "Batch: 88, Loss: 0.00013775039406027645\n",
      "Batch: 92, Loss: 3.608362749218941e-05\n",
      "Batch: 96, Loss: 2.6281272766937036e-06\n",
      "Batch: 100, Loss: 0.000568896415643394\n",
      "Batch: 104, Loss: 3.9674182517046575e-07\n",
      "Batch: 108, Loss: 0.00013677777315024287\n",
      "Batch: 112, Loss: 0.04988781362771988\n",
      "Batch: 116, Loss: 0.002231115009635687\n",
      "Batch: 120, Loss: 0.002884240821003914\n",
      "Batch: 124, Loss: 0.00261466926895082\n",
      "Batch: 128, Loss: 7.417029701173306e-05\n",
      "Batch: 132, Loss: 0.00025566635304130614\n",
      "Batch: 136, Loss: 5.699299435946159e-06\n",
      "Batch: 140, Loss: 1.6864436474861577e-05\n",
      "Batch: 144, Loss: 0.0031888883095234632\n",
      "Batch: 148, Loss: 1.4072784324525855e-05\n",
      "Batch: 152, Loss: 0.0034104599617421627\n",
      "Batch: 156, Loss: 4.1514555050525814e-05\n",
      "Batch: 160, Loss: 4.201677711535012e-06\n",
      "Batch: 164, Loss: 1.5623845683876425e-05\n",
      "Batch: 168, Loss: 0.00020685812341980636\n",
      "Batch: 172, Loss: 1.0401761755929329e-05\n",
      "Batch: 176, Loss: 0.0004202784039080143\n",
      "Batch: 180, Loss: 9.242001397069544e-05\n",
      "Batch: 184, Loss: 3.2445157103211386e-06\n",
      "Batch: 188, Loss: 0.00016139174113050103\n",
      "Batch: 192, Loss: 0.0008859043009579182\n",
      "Batch: 196, Loss: 4.499547685554717e-06\n",
      "Batch: 200, Loss: 0.0037221647799015045\n",
      "Batch: 204, Loss: 4.1854786104522645e-05\n",
      "Batch: 208, Loss: 0.00010468935943208635\n",
      "Batch: 212, Loss: 0.0016323893796652555\n",
      "Batch: 216, Loss: 0.0029763421043753624\n",
      "Batch: 220, Loss: 7.800626190146431e-05\n",
      "Batch: 224, Loss: 0.0020374194718897343\n",
      "Batch: 228, Loss: 1.3112842225382337e-06\n",
      "Batch: 232, Loss: 4.323703615227714e-05\n",
      "Batch: 236, Loss: 5.21496531291632e-06\n",
      "Batch: 240, Loss: 5.088208581582876e-06\n",
      "Batch: 244, Loss: 0.0007805376080796123\n",
      "Batch: 248, Loss: 0.0007162265246734023\n",
      "Batch: 252, Loss: 0.03762665018439293\n",
      "Batch: 256, Loss: 5.252410483080894e-05\n",
      "Batch: 260, Loss: 4.095440544915618e-06\n",
      "Batch: 264, Loss: 6.293401384027675e-05\n",
      "Batch: 268, Loss: 1.316874886470032e-06\n",
      "Batch: 272, Loss: 9.483451140113175e-05\n",
      "Batch: 276, Loss: 0.0009367410675622523\n",
      "Batch: 280, Loss: 0.0030073763336986303\n",
      "Batch: 284, Loss: 8.21193098090589e-05\n",
      "Batch: 288, Loss: 0.0003193824377376586\n",
      "Batch: 292, Loss: 0.00036026028101332486\n",
      "Batch: 296, Loss: 0.00013265362940728664\n",
      "Batch: 300, Loss: 0.005499416030943394\n",
      "Batch: 304, Loss: 0.0007650727638974786\n",
      "Batch: 308, Loss: 0.03824053332209587\n",
      "Batch: 312, Loss: 0.0002748905390035361\n",
      "Batch: 316, Loss: 1.4901141298651055e-07\n",
      "Batch: 320, Loss: 0.000295824371278286\n",
      "Batch: 324, Loss: 0.0006876791012473404\n",
      "Batch: 328, Loss: 0.0002564420865382999\n",
      "Batch: 332, Loss: 0.00010063756781164557\n",
      "Batch: 336, Loss: 3.084329728153534e-05\n",
      "Batch: 340, Loss: 0.00013316086551640183\n",
      "Batch: 344, Loss: 0.00013682742428500205\n",
      "Batch: 348, Loss: 3.053593172808178e-05\n",
      "Batch: 352, Loss: 0.02153320237994194\n",
      "Batch: 356, Loss: 5.177823823032668e-06\n",
      "Batch: 360, Loss: 3.91491485061124e-05\n",
      "Batch: 364, Loss: 0.01521117053925991\n",
      "Batch: 368, Loss: 2.5822306270129047e-05\n",
      "Batch: 372, Loss: 0.0004515482287388295\n",
      "Batch: 376, Loss: 0.00011958838149439543\n",
      "Batch: 380, Loss: 5.960416160633031e-07\n",
      "Batch: 384, Loss: 0.0001710049546090886\n",
      "Batch: 388, Loss: 2.7901321573153837e-06\n",
      "Batch: 392, Loss: 0.00018220252241007984\n",
      "Batch: 396, Loss: 5.493364733410999e-05\n",
      "Batch: 400, Loss: 4.187243030173704e-05\n",
      "Batch: 404, Loss: 1.9557725750019017e-07\n",
      "Batch: 408, Loss: 0.00013409947860054672\n",
      "Batch: 412, Loss: 2.7800964744528756e-05\n",
      "Batch: 416, Loss: 3.9490572817157954e-05\n",
      "Batch: 420, Loss: 0.004515346605330706\n",
      "Batch: 424, Loss: 0.0019085557432845235\n",
      "Batch: 428, Loss: 3.279590237070806e-05\n",
      "Batch: 432, Loss: 0.003788936184719205\n",
      "Batch: 436, Loss: 0.00023438376956619322\n",
      "Batch: 440, Loss: 6.360747647704557e-05\n",
      "Batch: 444, Loss: 0.003981774672865868\n",
      "Batch: 448, Loss: 2.1047850395916612e-07\n",
      "Batch: 452, Loss: 0.0021286176051944494\n",
      "Batch: 456, Loss: 0.00013592327013611794\n",
      "Batch: 460, Loss: 0.002505758311599493\n",
      "Batch: 464, Loss: 0.0011720748152583838\n",
      "Batch: 468, Loss: 0.00013551762094721198\n",
      "Batch: 472, Loss: 3.071744504268281e-05\n",
      "Batch: 476, Loss: 5.293194408295676e-05\n",
      "Batch: 480, Loss: 1.5499275832553394e-05\n",
      "Batch: 484, Loss: 0.00023636409605387598\n",
      "Batch: 488, Loss: 1.0989592880150667e-07\n",
      "Batch: 492, Loss: 6.085686618462205e-05\n",
      "Batch: 496, Loss: 2.7717063858290203e-05\n",
      "Batch: 500, Loss: 0.0017590749775990844\n",
      "Batch: 504, Loss: 8.758438343647867e-05\n",
      "Batch: 508, Loss: 0.006409900262951851\n",
      "Batch: 512, Loss: 8.566666110709775e-06\n",
      "Batch: 516, Loss: 0.05881113559007645\n",
      "Batch: 520, Loss: 2.3469164034395362e-07\n",
      "Batch: 524, Loss: 0.020878955721855164\n",
      "Batch: 528, Loss: 0.018822386860847473\n",
      "Batch: 532, Loss: 1.6316137134708697e-06\n",
      "Batch: 536, Loss: 0.004947980400174856\n",
      "Batch: 540, Loss: 2.3655508130104863e-07\n",
      "Batch: 544, Loss: 2.726830416577286e-06\n",
      "Batch: 548, Loss: 1.829398206609767e-05\n",
      "Batch: 552, Loss: 9.570324618835002e-05\n",
      "Batch: 556, Loss: 0.000848931900691241\n",
      "Batch: 560, Loss: 0.0032474712934345007\n",
      "Batch: 564, Loss: 0.003003173042088747\n",
      "Batch: 568, Loss: 0.00027714899624697864\n",
      "Batch: 572, Loss: 0.0017945792060345411\n",
      "Batch: 576, Loss: 4.286620605853386e-05\n",
      "Batch: 580, Loss: 5.010460881749168e-07\n",
      "Batch: 584, Loss: 0.00035861830110661685\n",
      "Batch: 588, Loss: 5.508567483047955e-05\n",
      "Batch: 592, Loss: 8.791626555648691e-07\n",
      "Batch: 596, Loss: 0.00013334371033124626\n",
      "Batch: 600, Loss: 0.005968204699456692\n",
      "Batch: 604, Loss: 0.00017732975538820028\n",
      "Batch: 608, Loss: 0.0004054569290019572\n",
      "Batch: 612, Loss: 0.0002285478258272633\n",
      "Batch: 616, Loss: 4.434703896549763e-06\n",
      "Batch: 620, Loss: 9.829153714235872e-05\n",
      "Batch: 624, Loss: 0.00010460714111104608\n",
      "Batch: 628, Loss: 0.002026026602834463\n",
      "Batch: 632, Loss: 2.9615262974402867e-05\n",
      "Batch: 636, Loss: 0.0003039799630641937\n",
      "Batch: 640, Loss: 0.008263514377176762\n",
      "Batch: 644, Loss: 2.0618997496058e-06\n",
      "Batch: 648, Loss: 3.900198407791322e-06\n",
      "Batch: 652, Loss: 1.076712942449376e-05\n",
      "Batch: 656, Loss: 0.00043797557009384036\n",
      "Batch: 660, Loss: 1.6269206753349863e-05\n",
      "Batch: 664, Loss: 0.00010210956679657102\n",
      "Batch: 668, Loss: 5.200454324949533e-05\n",
      "Batch: 672, Loss: 0.00031323713483288884\n",
      "Batch: 676, Loss: 0.0069043925032019615\n",
      "Batch: 680, Loss: 0.0025214143097400665\n",
      "Batch: 684, Loss: 7.187041774159297e-06\n",
      "Batch: 688, Loss: 2.9911877845734125e-06\n",
      "Batch: 692, Loss: 0.00013226513692643493\n",
      "Batch: 696, Loss: 0.10256236046552658\n",
      "Batch: 700, Loss: 0.0007258742698468268\n",
      "Batch: 704, Loss: 1.5132997759792488e-05\n",
      "Batch: 708, Loss: 3.7260866520227864e-05\n",
      "Batch: 712, Loss: 0.057257987558841705\n",
      "Batch: 716, Loss: 0.005154978483915329\n",
      "Batch: 720, Loss: 0.061763279139995575\n",
      "Batch: 724, Loss: 2.326341473235516e-06\n",
      "Batch: 728, Loss: 1.5869563867454417e-06\n",
      "Batch: 732, Loss: 0.007629153318703175\n",
      "Batch: 736, Loss: 0.00019261843408457935\n",
      "Batch: 740, Loss: 0.0004654249351006001\n",
      "Batch: 744, Loss: 4.349006303527858e-06\n",
      "Batch: 748, Loss: 8.808668098936323e-06\n",
      "Batch: 752, Loss: 2.167375714634545e-05\n",
      "Batch: 756, Loss: 3.103144990745932e-05\n",
      "Batch: 760, Loss: 0.0031639456283301115\n",
      "Batch: 764, Loss: 1.3460217815008946e-05\n",
      "Batch: 768, Loss: 1.4315508451545611e-05\n",
      "Batch: 772, Loss: 0.005270455963909626\n",
      "Batch: 776, Loss: 0.0006316900835372508\n",
      "Batch: 780, Loss: 4.313948375056498e-05\n",
      "Batch: 784, Loss: 0.0005791083676740527\n",
      "Batch: 788, Loss: 4.930101567879319e-06\n",
      "Batch: 792, Loss: 0.00282388087362051\n",
      "Batch: 796, Loss: 1.0604956514725927e-05\n",
      "Batch: 800, Loss: 5.6903572840383276e-05\n",
      "Batch: 804, Loss: 0.006080606020987034\n",
      "Batch: 808, Loss: 0.0010743408929556608\n",
      "Batch: 812, Loss: 0.053694628179073334\n",
      "Batch: 816, Loss: 2.3399807105306536e-05\n",
      "Batch: 820, Loss: 0.0007792191463522613\n",
      "Batch: 824, Loss: 0.030576376244425774\n",
      "Batch: 828, Loss: 5.0397379709465895e-06\n",
      "Batch: 832, Loss: 5.2439532737480476e-05\n",
      "Batch: 836, Loss: 0.02248550020158291\n",
      "Batch: 840, Loss: 0.06293711811304092\n",
      "Batch: 844, Loss: 0.00038890191353857517\n",
      "Batch: 848, Loss: 4.4179334508953616e-05\n",
      "Batch: 852, Loss: 6.299112101260107e-06\n",
      "Batch: 856, Loss: 5.541815698961727e-05\n",
      "Batch: 860, Loss: 4.1536597450431145e-07\n",
      "Batch: 864, Loss: 2.9085080313961953e-05\n",
      "Batch: 868, Loss: 0.006990601308643818\n",
      "Batch: 872, Loss: 0.00512368930503726\n",
      "Batch: 876, Loss: 5.2154042862184724e-08\n",
      "Batch: 880, Loss: 2.9689306302316254e-06\n",
      "Batch: 884, Loss: 1.6692118151695468e-05\n",
      "Batch: 888, Loss: 4.4703097046294715e-07\n",
      "Batch: 892, Loss: 0.02749601937830448\n",
      "Batch: 896, Loss: 0.10204877704381943\n",
      "Batch: 900, Loss: 0.02192636765539646\n",
      "Batch: 904, Loss: 0.0024839909747242928\n",
      "Batch: 908, Loss: 2.320364183105994e-05\n",
      "Batch: 912, Loss: 6.853098057035822e-06\n",
      "Batch: 916, Loss: 2.1217947505647317e-05\n",
      "Batch: 920, Loss: 0.0003732897457666695\n",
      "Batch: 924, Loss: 0.00010456427844474092\n",
      "Batch: 928, Loss: 0.017434624955058098\n",
      "Batch: 932, Loss: 0.0037514122668653727\n",
      "Batch: 936, Loss: 0.0021758771035820246\n",
      "\n",
      "验证集: Average loss: 0.0600, Accuracy: 9891/10000 (99%)\n",
      "\n",
      "Model weights saved to logs/model_weights_epoch8.pth\n",
      "Epoch 10\n",
      "Batch: 0, Loss: 1.2814758747481392e-06\n",
      "Batch: 4, Loss: 0.0005563849117606878\n",
      "Batch: 8, Loss: 0.0016325422329828143\n",
      "Batch: 12, Loss: 0.00032746369834057987\n",
      "Batch: 16, Loss: 2.533187455355801e-07\n",
      "Batch: 20, Loss: 0.00022147463459987193\n",
      "Batch: 24, Loss: 9.096512258111034e-06\n",
      "Batch: 28, Loss: 4.823787548957625e-06\n",
      "Batch: 32, Loss: 1.540862285764888e-05\n",
      "Batch: 36, Loss: 8.605290986452019e-07\n",
      "Batch: 40, Loss: 5.2051156671950594e-05\n",
      "Batch: 44, Loss: 4.125465238757897e-06\n",
      "Batch: 48, Loss: 8.639770385343581e-06\n",
      "Batch: 52, Loss: 0.0001204203799716197\n",
      "Batch: 56, Loss: 3.6880101106362417e-07\n",
      "Batch: 60, Loss: 0.04479753226041794\n",
      "Batch: 64, Loss: 0.00039641253533773124\n",
      "Batch: 68, Loss: 5.205543402553303e-06\n",
      "Batch: 72, Loss: 3.7066490676807007e-07\n",
      "Batch: 76, Loss: 0.0012305463897064328\n",
      "Batch: 80, Loss: 3.1490471883444116e-05\n",
      "Batch: 84, Loss: 5.007911749999039e-05\n",
      "Batch: 88, Loss: 5.0772585382219404e-05\n",
      "Batch: 92, Loss: 5.224423603067407e-06\n",
      "Batch: 96, Loss: 7.402912888210267e-05\n",
      "Batch: 100, Loss: 7.674016160308383e-07\n",
      "Batch: 104, Loss: 3.2574762371950783e-06\n",
      "Batch: 108, Loss: 2.449269913995522e-06\n",
      "Batch: 112, Loss: 8.657860598759726e-05\n",
      "Batch: 116, Loss: 9.145394415099872e-07\n",
      "Batch: 120, Loss: 0.00022470246767625213\n",
      "Batch: 124, Loss: 1.4208481843525078e-05\n",
      "Batch: 128, Loss: 1.649436671868898e-05\n",
      "Batch: 132, Loss: 0.00018580704636406153\n",
      "Batch: 136, Loss: 2.4753510388109135e-06\n",
      "Batch: 140, Loss: 2.5088775146286935e-06\n",
      "Batch: 144, Loss: 0.007548903580754995\n",
      "Batch: 148, Loss: 1.1237112630624324e-05\n",
      "Batch: 152, Loss: 9.638895426178351e-05\n",
      "Batch: 156, Loss: 0.0042807054705917835\n",
      "Batch: 160, Loss: 3.506960274535231e-05\n",
      "Batch: 164, Loss: 6.918828148627654e-06\n",
      "Batch: 168, Loss: 9.611151199351298e-07\n",
      "Batch: 172, Loss: 8.66950267663924e-06\n",
      "Batch: 176, Loss: 3.33956595568452e-05\n",
      "Batch: 180, Loss: 1.634894942981191e-05\n",
      "Batch: 184, Loss: 5.51558805454988e-05\n",
      "Batch: 188, Loss: 0.0038373167626559734\n",
      "Batch: 192, Loss: 0.0004853021528106183\n",
      "Batch: 196, Loss: 2.4711369405849837e-05\n",
      "Batch: 200, Loss: 0.05714796110987663\n",
      "Batch: 204, Loss: 1.4077861123951152e-05\n",
      "Batch: 208, Loss: 0.0005861591198481619\n",
      "Batch: 212, Loss: 3.1193994800560176e-05\n",
      "Batch: 216, Loss: 0.00017069796740543097\n",
      "Batch: 220, Loss: 0.001067246892489493\n",
      "Batch: 224, Loss: 0.05838187038898468\n",
      "Batch: 228, Loss: 1.8271524822921492e-05\n",
      "Batch: 232, Loss: 6.277082889027952e-07\n",
      "Batch: 236, Loss: 5.2279515330155846e-06\n",
      "Batch: 240, Loss: 9.015068940243509e-07\n",
      "Batch: 244, Loss: 8.856187196215615e-05\n",
      "Batch: 248, Loss: 0.0015259893843904138\n",
      "Batch: 252, Loss: 0.0018727751448750496\n",
      "Batch: 256, Loss: 2.9166317290219013e-06\n",
      "Batch: 260, Loss: 5.944918939349009e-06\n",
      "Batch: 264, Loss: 0.00031823967583477497\n",
      "Batch: 268, Loss: 8.261558832600713e-06\n",
      "Batch: 272, Loss: 1.620063449081499e-05\n",
      "Batch: 276, Loss: 6.477240276581142e-06\n",
      "Batch: 280, Loss: 1.2702741969405906e-06\n",
      "Batch: 284, Loss: 1.266594296112089e-07\n",
      "Batch: 288, Loss: 0.01670803315937519\n",
      "Batch: 292, Loss: 7.43216014598147e-06\n",
      "Batch: 296, Loss: 2.955800482595805e-06\n",
      "Batch: 300, Loss: 3.4505043004173785e-05\n",
      "Batch: 304, Loss: 5.553858045459492e-06\n",
      "Batch: 308, Loss: 3.762513074434537e-07\n",
      "Batch: 312, Loss: 2.760760071396362e-05\n",
      "Batch: 316, Loss: 7.823093284287097e-08\n",
      "Batch: 320, Loss: 2.7043299724027747e-06\n",
      "Batch: 324, Loss: 0.04190010949969292\n",
      "Batch: 328, Loss: 6.858484266558662e-05\n",
      "Batch: 332, Loss: 0.026942668482661247\n",
      "Batch: 336, Loss: 2.114983362844214e-05\n",
      "Batch: 340, Loss: 2.257439746244927e-06\n",
      "Batch: 344, Loss: 6.769777428417001e-06\n",
      "Batch: 348, Loss: 3.9860162814875366e-07\n",
      "Batch: 352, Loss: 0.0001357957225991413\n",
      "Batch: 356, Loss: 0.0010598540538921952\n",
      "Batch: 360, Loss: 2.05622854991816e-05\n",
      "Batch: 364, Loss: 8.456850082438905e-06\n",
      "Batch: 368, Loss: 0.006296447943896055\n",
      "Batch: 372, Loss: 0.0020070220343768597\n",
      "Batch: 376, Loss: 0.018858522176742554\n",
      "Batch: 380, Loss: 2.054407104878919e-06\n",
      "Batch: 384, Loss: 5.317579780239612e-05\n",
      "Batch: 388, Loss: 0.004948971327394247\n",
      "Batch: 392, Loss: 1.172908832813846e-05\n",
      "Batch: 396, Loss: 1.380752928525908e-05\n",
      "Batch: 400, Loss: 0.10169511288404465\n",
      "Batch: 404, Loss: 6.073391887184698e-06\n",
      "Batch: 408, Loss: 1.6204961639232351e-07\n",
      "Batch: 412, Loss: 1.2798699572158512e-05\n",
      "Batch: 416, Loss: 6.159322310850257e-06\n",
      "Batch: 420, Loss: 2.2724178450062027e-07\n",
      "Batch: 424, Loss: 2.4878025215002708e-05\n",
      "Batch: 428, Loss: 3.650769713203772e-07\n",
      "Batch: 432, Loss: 0.00021981896134093404\n",
      "Batch: 436, Loss: 6.698721790598938e-06\n",
      "Batch: 440, Loss: 5.152849553269334e-05\n",
      "Batch: 444, Loss: 1.868014624051284e-05\n",
      "Batch: 448, Loss: 1.8580331015982665e-05\n",
      "Batch: 452, Loss: 2.0463317923713475e-05\n",
      "Batch: 456, Loss: 2.2724221082626173e-07\n",
      "Batch: 460, Loss: 0.00022249325411394238\n",
      "Batch: 464, Loss: 4.231559159961762e-06\n",
      "Batch: 468, Loss: 0.004046857822686434\n",
      "Batch: 472, Loss: 4.619342917067115e-07\n",
      "Batch: 476, Loss: 3.358063622727059e-06\n",
      "Batch: 480, Loss: 4.534476829576306e-05\n",
      "Batch: 484, Loss: 0.008888582699000835\n",
      "Batch: 488, Loss: 1.1774859558499884e-05\n",
      "Batch: 492, Loss: 0.006711936555802822\n",
      "Batch: 496, Loss: 0.0017041462706401944\n",
      "Batch: 500, Loss: 2.2946712761040544e-06\n",
      "Batch: 504, Loss: 0.0006639197818003595\n",
      "Batch: 508, Loss: 0.3252396583557129\n",
      "Batch: 512, Loss: 1.2317064829403535e-05\n",
      "Batch: 516, Loss: 0.00022950186394155025\n",
      "Batch: 520, Loss: 1.2146384506195318e-05\n",
      "Batch: 524, Loss: 0.013803621754050255\n",
      "Batch: 528, Loss: 2.3604254238307476e-05\n",
      "Batch: 532, Loss: 0.0004424086946528405\n",
      "Batch: 536, Loss: 0.004812038037925959\n",
      "Batch: 540, Loss: 0.00733058899641037\n",
      "Batch: 544, Loss: 1.6428357412223704e-06\n",
      "Batch: 548, Loss: 1.0860640941245947e-05\n",
      "Batch: 552, Loss: 0.0006993038114160299\n",
      "Batch: 556, Loss: 8.884624094207538e-07\n",
      "Batch: 560, Loss: 3.659732828964479e-05\n",
      "Batch: 564, Loss: 9.799723557080142e-06\n",
      "Batch: 568, Loss: 0.00022560371144209057\n",
      "Batch: 572, Loss: 0.00010369328811066225\n",
      "Batch: 576, Loss: 3.6859769352304284e-06\n",
      "Batch: 580, Loss: 0.0014047614531591535\n",
      "Batch: 584, Loss: 0.02029045671224594\n",
      "Batch: 588, Loss: 0.0017481007380411029\n",
      "Batch: 592, Loss: 0.00011982420255662873\n",
      "Batch: 596, Loss: 0.0036511768121272326\n",
      "Batch: 600, Loss: 4.3958226569884573e-07\n",
      "Batch: 604, Loss: 0.00016864296048879623\n",
      "Batch: 608, Loss: 4.598220129992114e-06\n",
      "Batch: 612, Loss: 0.003193820361047983\n",
      "Batch: 616, Loss: 3.19592327286955e-05\n",
      "Batch: 620, Loss: 4.442662975634448e-05\n",
      "Batch: 624, Loss: 1.1143340088892728e-05\n",
      "Batch: 628, Loss: 0.0009390675695613027\n",
      "Batch: 632, Loss: 0.00015130714746192098\n",
      "Batch: 636, Loss: 0.0004573488258756697\n",
      "Batch: 640, Loss: 0.0004890203126706183\n",
      "Batch: 644, Loss: 0.010402612388134003\n",
      "Batch: 648, Loss: 5.140865368957748e-07\n",
      "Batch: 652, Loss: 0.01788780465722084\n",
      "Batch: 656, Loss: 8.98998769116588e-05\n",
      "Batch: 660, Loss: 1.5715557310613804e-05\n",
      "Batch: 664, Loss: 0.0031345856841653585\n",
      "Batch: 668, Loss: 0.0009861016878858209\n",
      "Batch: 672, Loss: 2.248982673336286e-05\n",
      "Batch: 676, Loss: 0.005197295919060707\n",
      "Batch: 680, Loss: 8.284283831017092e-05\n",
      "Batch: 684, Loss: 8.291409903904423e-05\n",
      "Batch: 688, Loss: 0.0001960827357834205\n",
      "Batch: 692, Loss: 3.49965216628334e-06\n",
      "Batch: 696, Loss: 0.19066303968429565\n",
      "Batch: 700, Loss: 5.327140115696238e-07\n",
      "Batch: 704, Loss: 0.01176979299634695\n",
      "Batch: 708, Loss: 0.0012653432786464691\n",
      "Batch: 712, Loss: 0.007462787441909313\n",
      "Batch: 716, Loss: 3.1539246265310794e-05\n",
      "Batch: 720, Loss: 0.0009779247920960188\n",
      "Batch: 724, Loss: 0.0005463859997689724\n",
      "Batch: 728, Loss: 6.544422376464354e-06\n",
      "Batch: 732, Loss: 0.01009781938046217\n",
      "Batch: 736, Loss: 4.834678111365065e-05\n",
      "Batch: 740, Loss: 6.965766806388274e-05\n",
      "Batch: 744, Loss: 0.0010677359532564878\n",
      "Batch: 748, Loss: 0.0007666719029657543\n",
      "Batch: 752, Loss: 1.6297222828143276e-05\n",
      "Batch: 756, Loss: 0.0013393175322562456\n",
      "Batch: 760, Loss: 0.0013961122604086995\n",
      "Batch: 764, Loss: 0.0034999458584934473\n",
      "Batch: 768, Loss: 0.0006940244929865003\n",
      "Batch: 772, Loss: 0.0013832723489031196\n",
      "Batch: 776, Loss: 0.0002658891025930643\n",
      "Batch: 780, Loss: 2.54401547863381e-05\n",
      "Batch: 784, Loss: 0.0003219424397684634\n",
      "Batch: 788, Loss: 0.0007230413029901683\n",
      "Batch: 792, Loss: 0.04035639390349388\n",
      "Batch: 796, Loss: 3.72979229723569e-05\n",
      "Batch: 800, Loss: 8.391931623918936e-05\n",
      "Batch: 804, Loss: 7.330552762141451e-06\n",
      "Batch: 808, Loss: 0.023123789578676224\n",
      "Batch: 812, Loss: 5.762040018453263e-06\n",
      "Batch: 816, Loss: 0.008729197084903717\n",
      "Batch: 820, Loss: 1.5348065289799706e-06\n",
      "Batch: 824, Loss: 7.464672671630979e-05\n",
      "Batch: 828, Loss: 5.010497829971428e-07\n",
      "Batch: 832, Loss: 0.056574586778879166\n",
      "Batch: 836, Loss: 9.217459592036903e-06\n",
      "Batch: 840, Loss: 6.160109478514642e-05\n",
      "Batch: 844, Loss: 0.005655041430145502\n",
      "Batch: 848, Loss: 0.004289328586310148\n",
      "Batch: 852, Loss: 6.872070662211627e-05\n",
      "Batch: 856, Loss: 0.00028705166187137365\n",
      "Batch: 860, Loss: 5.768146365880966e-06\n",
      "Batch: 864, Loss: 6.432910140574677e-06\n",
      "Batch: 868, Loss: 1.559024167363532e-05\n",
      "Batch: 872, Loss: 0.003432672470808029\n",
      "Batch: 876, Loss: 4.394445932121016e-05\n",
      "Batch: 880, Loss: 5.09967094330932e-06\n",
      "Batch: 884, Loss: 0.00034771763603203\n",
      "Batch: 888, Loss: 6.546619260916486e-06\n",
      "Batch: 892, Loss: 5.122227548781666e-07\n",
      "Batch: 896, Loss: 0.016493167728185654\n",
      "Batch: 900, Loss: 0.00024154921993613243\n",
      "Batch: 904, Loss: 6.51917616778519e-07\n",
      "Batch: 908, Loss: 0.0003467027563601732\n",
      "Batch: 912, Loss: 0.00010500242933630943\n",
      "Batch: 916, Loss: 1.948298177012475e-06\n",
      "Batch: 920, Loss: 8.006783900782466e-06\n",
      "Batch: 924, Loss: 1.4924830793461297e-05\n",
      "Batch: 928, Loss: 4.44371562480228e-06\n",
      "Batch: 932, Loss: 1.79204889718676e-05\n",
      "Batch: 936, Loss: 0.0006187277031131089\n",
      "\n",
      "验证集: Average loss: 0.0573, Accuracy: 9894/10000 (99%)\n",
      "\n",
      "Model weights saved to logs/model_weights_epoch9.pth\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    train(net, device_torch, train_loader, optimizer)\n",
    "    test(net, device_torch, test_loader)\n",
    "    model_save_path = r\"logs/model_weights_epoch\" + str(epoch) + \".pth\"\n",
    "    torch.save(net.state_dict(), model_save_path)\n",
    "    print(f'Model weights saved to {model_save_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7efc8a8c57f0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
